#! /usr/bin/env perl
# Copyright 2010-2018 The OpenSSL Project Authors. All Rights Reserved.
#
# Licensed under the OpenSSL license (the "License").  You may not use
# this file except in compliance with the License.  You can obtain a copy
# in the file LICENSE in the source distribution or at
# https://www.openssl.org/source/license.html


# ====================================================================
# Written by Andy Polyakov <appro@openssl.org> for the OpenSSL
# project. The module is, however, dual licensed under OpenSSL and
# CRYPTOGAMS licenses depending on where you obtain it. For further
# details see http://www.openssl.org/~appro/cryptogams/.
# ====================================================================

# AES for MIPS

# October 2010
#
# Code uses 1K[+256B] S-box and on single-issue core [such as R5000]
# spends ~68 cycles per byte processed with 128-bit key. This is ~16%
# faster than gcc-generated code, which is not very impressive. But
# recall that compressed S-box requires extra processing, namely
# additional rotations. Rotations are implemented with lwl/lwr pairs,
# which is normally used for loading unaligned data. Another cool
# thing about this module is its endian neutrality, which means that
# it processes data without ever changing byte order...

# September 2012
#
# Add MIPS32R2 (~10% less instructions) and SmartMIPS ASE (further
# ~25% less instructions) code. Note that there is no run-time switch,
# instead, code path is chosen upon pre-process time, pass -mips32r2
# or/and -msmartmips.

######################################################################
# There is a number of MIPS ABI in use, O32 and N32/64 are most
# widely used. Then there is a new contender: NUBI. It appears that if
# one picks the latter, it's possible to arrange code in ABI neutral
# manner. Therefore let's stick to NUBI register layout:
#
($zero,$at,$t0,$t1,$t2)=map("\$$_",(0..2,24,25));
($a0,$a1,$a2,$a3,$a4,$a5,$a6,$a7)=map("\$$_",(4..11));
($s0,$s1,$s2,$s3,$s4,$s5,$s6,$s7,$s8,$s9,$s10,$s11)=map("\$$_",(12..23));
($gp,$tp,$sp,$fp,$ra)=map("\$$_",(3,28..31));
#
# The return value is placed in $a0. Following coding rules facilitate
# interoperability:
#
# - never ever touch $tp, "thread pointer", former $gp;
# - copy return value to $t0, former $v0 [or to $a0 if you're adapting
#   old code];
# - on O32 populate $a4-$a7 with 'lw $aN,4*N($sp)' if necessary;
#
# For reference here is register layout for N32/64 MIPS ABIs:
#
# ($zero,$at,$v0,$v1)=map("\$$_",(0..3));
# ($a0,$a1,$a2,$a3,$a4,$a5,$a6,$a7)=map("\$$_",(4..11));
# ($t0,$t1,$t2,$t3,$t8,$t9)=map("\$$_",(12..15,24,25));
# ($s0,$s1,$s2,$s3,$s4,$s5,$s6,$s7)=map("\$$_",(16..23));
# ($gp,$sp,$fp,$ra)=map("\$$_",(28..31));
#
$flavour = shift || "o32"; # supported flavours are o32,n32,64,nubi32,nubi64

if ($flavour =~ /64|n32/i) {
	$PTR_LA="dla";
	$PTR_ADD="daddu";	# incidentally works even on n32
	$PTR_SUB="dsubu";	# incidentally works even on n32
	$PTR_INS="dins";
	$REG_S="sd";
	$REG_L="ld";
	$PTR_SLL="dsll";	# incidentally works even on n32
	$SZREG=8;
} else {
	$PTR_LA="la";
	$PTR_ADD="addu";
	$PTR_SUB="subu";
	$PTR_INS="ins";
	$REG_S="sw";
	$REG_L="lw";
	$PTR_SLL="sll";
	$SZREG=4;
}
$pf = ($flavour =~ /nubi/i) ? $t0 : $t2;
#
# <appro@openssl.org>
#
######################################################################

$big_endian=(`echo MIPSEB | $ENV{CC} -E -`=~/MIPSEB/)?0:1 if ($ENV{CC});

for (@ARGV) {	$output=$_ if (/\w[\w\-]*\.\w+$/);	}
open STDOUT,">$output";

if (!defined($big_endian))
{    $big_endian=(unpack('L',pack('N',1))==1);   }

while (($output=shift) && ($output!~/\w[\w\-]*\.\w+$/)) {}
open STDOUT,">$output";

my ($MSB,$LSB)=(0,3);	# automatically converted to little-endian

$code.=<<___;
#include "mips_arch.h"

.text
#if !defined(__mips_eabi) && (!defined(__vxworks) || defined(__pic__))
.option	pic2
#endif
.set	noat
___

{{{
my $FRAMESIZE=16*$SZREG;
my $SAVED_REGS_MASK = ($flavour =~ /nubi/i) ? "0xFF" : "0xFF";

my ($inp,$out,$key,$Tbl,$s0,$s1,$s2,$s3)=($a0,$a1,$a2,$a3,$a4,$a5,$a6,$a7);
my ($i0,$i1,$i2,$i3)=($at,$t0,$t1,$t2);
my ($t0,$t1,$t2,$t3,$t4,$t5,$t6,$t7,$t8,$t9,$t10,$t11) = map("\$$_",(12..23));
my ($key0,$cnt)=($gp,$fp);

# instruction ordering is "stolen" from output from MIPSpro assembler
# invoked with -mips3 -O3 arguments...
$code.=<<___;
.align	5
.ent	_mips_AES_encrypt
_mips_AES_encrypt:
	.frame	$sp,0,$ra
	.set	reorder
	lw	$t0,0($key)
	lw	$t1,4($key)
	lw	$t2,8($key)
	lw	$t3,12($key)
	lw	$cnt,240($key)
	$PTR_ADD $key0,$key,16

	xor	$s0,$t0
	xor	$s1,$t1
	xor	$s2,$t2
	xor	$s3,$t3

	subu	$cnt,1
#if defined(__mips_smartmips)
	ext	$i0,$s1,16,8
.Loop_enc:
	ext	$i1,$s2,16,8
	ext	$i2,$s3,16,8
	ext	$i3,$s0,16,8
	lwxs	$t0,$i0($Tbl)		# Te1[s1>>16]
	ext	$i0,$s2,8,8
	lwxs	$t1,$i1($Tbl)		# Te1[s2>>16]
	ext	$i1,$s3,8,8
	lwxs	$t2,$i2($Tbl)		# Te1[s3>>16]
	ext	$i2,$s0,8,8
	lwxs	$t3,$i3($Tbl)		# Te1[s0>>16]
	ext	$i3,$s1,8,8

	lwxs	$t4,$i0($Tbl)		# Te2[s2>>8]
	ext	$i0,$s3,0,8
	lwxs	$t5,$i1($Tbl)		# Te2[s3>>8]
	ext	$i1,$s0,0,8
	lwxs	$t6,$i2($Tbl)		# Te2[s0>>8]
	ext	$i2,$s1,0,8
	lwxs	$t7,$i3($Tbl)		# Te2[s1>>8]
	ext	$i3,$s2,0,8

	lwxs	$t8,$i0($Tbl)		# Te3[s3]
	ext	$i0,$s0,24,8
	lwxs	$t9,$i1($Tbl)		# Te3[s0]
	ext	$i1,$s1,24,8
	lwxs	$t10,$i2($Tbl)		# Te3[s1]
	ext	$i2,$s2,24,8
	lwxs	$t11,$i3($Tbl)		# Te3[s2]
	ext	$i3,$s3,24,8

	rotr	$t0,$t0,8
	rotr	$t1,$t1,8
	rotr	$t2,$t2,8
	rotr	$t3,$t3,8

	rotr	$t4,$t4,16
	rotr	$t5,$t5,16
	rotr	$t6,$t6,16
	rotr	$t7,$t7,16

	xor	$t0,$t4
	lwxs	$t4,$i0($Tbl)		# Te0[s0>>24]
	xor	$t1,$t5
	lwxs	$t5,$i1($Tbl)		# Te0[s1>>24]
	xor	$t2,$t6
	lwxs	$t6,$i2($Tbl)		# Te0[s2>>24]
	xor	$t3,$t7
	lwxs	$t7,$i3($Tbl)		# Te0[s3>>24]

	rotr	$t8,$t8,24
	lw	$s0,0($key0)
	rotr	$t9,$t9,24
	lw	$s1,4($key0)
	rotr	$t10,$t10,24
	lw	$s2,8($key0)
	rotr	$t11,$t11,24
	lw	$s3,12($key0)

	xor	$t0,$t8
	xor	$t1,$t9
	xor	$t2,$t10
	xor	$t3,$t11

	xor	$t0,$t4
	xor	$t1,$t5
	xor	$t2,$t6
	xor	$t3,$t7

	subu	$cnt,1
	$PTR_ADD $key0,16
	xor	$s0,$t0
	xor	$s1,$t1
	xor	$s2,$t2
	xor	$s3,$t3
	.set	noreorder
	bnez	$cnt,.Loop_enc
	ext	$i0,$s1,16,8

	_xtr	$i0,$s1,16-2
#else
	_xtr	$i0,$s1,16-2
.Loop_enc:
	_xtr	$i1,$s2,16-2
	_xtr	$i2,$s3,16-2
	_xtr	$i3,$s0,16-2
	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
#if defined(_MIPS_ARCH_MIPS32R2) || defined(_MIPS_ARCH_MIPS64R2)
	lw	$t0,0($i0)		# Te1[s1>>16]
	_xtr	$i0,$s2,8-2
	lw	$t1,0($i1)		# Te1[s2>>16]
	_xtr	$i1,$s3,8-2
	lw	$t2,0($i2)		# Te1[s3>>16]
	_xtr	$i2,$s0,8-2
	lw	$t3,0($i3)		# Te1[s0>>16]
	_xtr	$i3,$s1,8-2
#else
	lwl	$t0,3($i0)		# Te1[s1>>16]
	lwl	$t1,3($i1)		# Te1[s2>>16]
	lwl	$t2,3($i2)		# Te1[s3>>16]
	lwl	$t3,3($i3)		# Te1[s0>>16]
	lwr	$t0,2($i0)		# Te1[s1>>16]
	_xtr	$i0,$s2,8-2
	lwr	$t1,2($i1)		# Te1[s2>>16]
	_xtr	$i1,$s3,8-2
	lwr	$t2,2($i2)		# Te1[s3>>16]
	_xtr	$i2,$s0,8-2
	lwr	$t3,2($i3)		# Te1[s0>>16]
	_xtr	$i3,$s1,8-2
#endif
	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
#if defined(_MIPS_ARCH_MIPS32R2) || defined(_MIPS_ARCH_MIPS64R2)
	rotr	$t0,$t0,8
	rotr	$t1,$t1,8
	rotr	$t2,$t2,8
	rotr	$t3,$t3,8
# if defined(_MIPSEL)
	lw	$t4,0($i0)		# Te2[s2>>8]
	_xtr	$i0,$s3,0-2
	lw	$t5,0($i1)		# Te2[s3>>8]
	_xtr	$i1,$s0,0-2
	lw	$t6,0($i2)		# Te2[s0>>8]
	_xtr	$i2,$s1,0-2
	lw	$t7,0($i3)		# Te2[s1>>8]
	_xtr	$i3,$s2,0-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lw	$t8,0($i0)		# Te3[s3]
	$PTR_INS $i0,$s0,2,8
	lw	$t9,0($i1)		# Te3[s0]
	$PTR_INS $i1,$s1,2,8
	lw	$t10,0($i2)		# Te3[s1]
	$PTR_INS $i2,$s2,2,8
	lw	$t11,0($i3)		# Te3[s2]
	$PTR_INS $i3,$s3,2,8
# else
	lw	$t4,0($i0)		# Te2[s2>>8]
	$PTR_INS $i0,$s3,2,8
	lw	$t5,0($i1)		# Te2[s3>>8]
	$PTR_INS $i1,$s0,2,8
	lw	$t6,0($i2)		# Te2[s0>>8]
	$PTR_INS $i2,$s1,2,8
	lw	$t7,0($i3)		# Te2[s1>>8]
	$PTR_INS $i3,$s2,2,8

	lw	$t8,0($i0)		# Te3[s3]
	_xtr	$i0,$s0,24-2
	lw	$t9,0($i1)		# Te3[s0]
	_xtr	$i1,$s1,24-2
	lw	$t10,0($i2)		# Te3[s1]
	_xtr	$i2,$s2,24-2
	lw	$t11,0($i3)		# Te3[s2]
	_xtr	$i3,$s3,24-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
# endif
	rotr	$t4,$t4,16
	rotr	$t5,$t5,16
	rotr	$t6,$t6,16
	rotr	$t7,$t7,16

	rotr	$t8,$t8,24
	rotr	$t9,$t9,24
	rotr	$t10,$t10,24
	rotr	$t11,$t11,24
#else
	lwl	$t4,2($i0)		# Te2[s2>>8]
	lwl	$t5,2($i1)		# Te2[s3>>8]
	lwl	$t6,2($i2)		# Te2[s0>>8]
	lwl	$t7,2($i3)		# Te2[s1>>8]
	lwr	$t4,1($i0)		# Te2[s2>>8]
	_xtr	$i0,$s3,0-2
	lwr	$t5,1($i1)		# Te2[s3>>8]
	_xtr	$i1,$s0,0-2
	lwr	$t6,1($i2)		# Te2[s0>>8]
	_xtr	$i2,$s1,0-2
	lwr	$t7,1($i3)		# Te2[s1>>8]
	_xtr	$i3,$s2,0-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lwl	$t8,1($i0)		# Te3[s3]
	lwl	$t9,1($i1)		# Te3[s0]
	lwl	$t10,1($i2)		# Te3[s1]
	lwl	$t11,1($i3)		# Te3[s2]
	lwr	$t8,0($i0)		# Te3[s3]
	_xtr	$i0,$s0,24-2
	lwr	$t9,0($i1)		# Te3[s0]
	_xtr	$i1,$s1,24-2
	lwr	$t10,0($i2)		# Te3[s1]
	_xtr	$i2,$s2,24-2
	lwr	$t11,0($i3)		# Te3[s2]
	_xtr	$i3,$s3,24-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
#endif
	xor	$t0,$t4
	lw	$t4,0($i0)		# Te0[s0>>24]
	xor	$t1,$t5
	lw	$t5,0($i1)		# Te0[s1>>24]
	xor	$t2,$t6
	lw	$t6,0($i2)		# Te0[s2>>24]
	xor	$t3,$t7
	lw	$t7,0($i3)		# Te0[s3>>24]

	xor	$t0,$t8
	lw	$s0,0($key0)
	xor	$t1,$t9
	lw	$s1,4($key0)
	xor	$t2,$t10
	lw	$s2,8($key0)
	xor	$t3,$t11
	lw	$s3,12($key0)

	xor	$t0,$t4
	xor	$t1,$t5
	xor	$t2,$t6
	xor	$t3,$t7

	subu	$cnt,1
	$PTR_ADD $key0,16
	xor	$s0,$t0
	xor	$s1,$t1
	xor	$s2,$t2
	xor	$s3,$t3
	.set	noreorder
	bnez	$cnt,.Loop_enc
	_xtr	$i0,$s1,16-2
#endif

	.set	reorder
	_xtr	$i1,$s2,16-2
	_xtr	$i2,$s3,16-2
	_xtr	$i3,$s0,16-2
	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lbu	$t0,2($i0)		# Te4[s1>>16]
	_xtr	$i0,$s2,8-2
	lbu	$t1,2($i1)		# Te4[s2>>16]
	_xtr	$i1,$s3,8-2
	lbu	$t2,2($i2)		# Te4[s3>>16]
	_xtr	$i2,$s0,8-2
	lbu	$t3,2($i3)		# Te4[s0>>16]
	_xtr	$i3,$s1,8-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
#if defined(_MIPS_ARCH_MIPS32R2) || defined(_MIPS_ARCH_MIPS64R2)
# if defined(_MIPSEL)
	lbu	$t4,2($i0)		# Te4[s2>>8]
	$PTR_INS $i0,$s0,2,8
	lbu	$t5,2($i1)		# Te4[s3>>8]
	$PTR_INS $i1,$s1,2,8
	lbu	$t6,2($i2)		# Te4[s0>>8]
	$PTR_INS $i2,$s2,2,8
	lbu	$t7,2($i3)		# Te4[s1>>8]
	$PTR_INS $i3,$s3,2,8

	lbu	$t8,2($i0)		# Te4[s0>>24]
	_xtr	$i0,$s3,0-2
	lbu	$t9,2($i1)		# Te4[s1>>24]
	_xtr	$i1,$s0,0-2
	lbu	$t10,2($i2)		# Te4[s2>>24]
	_xtr	$i2,$s1,0-2
	lbu	$t11,2($i3)		# Te4[s3>>24]
	_xtr	$i3,$s2,0-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
# else
	lbu	$t4,2($i0)		# Te4[s2>>8]
	_xtr	$i0,$s0,24-2
	lbu	$t5,2($i1)		# Te4[s3>>8]
	_xtr	$i1,$s1,24-2
	lbu	$t6,2($i2)		# Te4[s0>>8]
	_xtr	$i2,$s2,24-2
	lbu	$t7,2($i3)		# Te4[s1>>8]
	_xtr	$i3,$s3,24-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lbu	$t8,2($i0)		# Te4[s0>>24]
	$PTR_INS $i0,$s3,2,8
	lbu	$t9,2($i1)		# Te4[s1>>24]
	$PTR_INS $i1,$s0,2,8
	lbu	$t10,2($i2)		# Te4[s2>>24]
	$PTR_INS $i2,$s1,2,8
	lbu	$t11,2($i3)		# Te4[s3>>24]
	$PTR_INS $i3,$s2,2,8
# endif
	_ins	$t0,16
	_ins	$t1,16
	_ins	$t2,16
	_ins	$t3,16

	_ins2	$t0,$t4,8
	lbu	$t4,2($i0)		# Te4[s3]
	_ins2	$t1,$t5,8
	lbu	$t5,2($i1)		# Te4[s0]
	_ins2	$t2,$t6,8
	lbu	$t6,2($i2)		# Te4[s1]
	_ins2	$t3,$t7,8
	lbu	$t7,2($i3)		# Te4[s2]

	_ins2	$t0,$t8,24
	lw	$s0,0($key0)
	_ins2	$t1,$t9,24
	lw	$s1,4($key0)
	_ins2	$t2,$t10,24
	lw	$s2,8($key0)
	_ins2	$t3,$t11,24
	lw	$s3,12($key0)

	_ins2	$t0,$t4,0
	_ins2	$t1,$t5,0
	_ins2	$t2,$t6,0
	_ins2	$t3,$t7,0
#else
	lbu	$t4,2($i0)		# Te4[s2>>8]
	_xtr	$i0,$s0,24-2
	lbu	$t5,2($i1)		# Te4[s3>>8]
	_xtr	$i1,$s1,24-2
	lbu	$t6,2($i2)		# Te4[s0>>8]
	_xtr	$i2,$s2,24-2
	lbu	$t7,2($i3)		# Te4[s1>>8]
	_xtr	$i3,$s3,24-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lbu	$t8,2($i0)		# Te4[s0>>24]
	_xtr	$i0,$s3,0-2
	lbu	$t9,2($i1)		# Te4[s1>>24]
	_xtr	$i1,$s0,0-2
	lbu	$t10,2($i2)		# Te4[s2>>24]
	_xtr	$i2,$s1,0-2
	lbu	$t11,2($i3)		# Te4[s3>>24]
	_xtr	$i3,$s2,0-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl

	_ins	$t0,16
	_ins	$t1,16
	_ins	$t2,16
	_ins	$t3,16

	_ins	$t4,8
	_ins	$t5,8
	_ins	$t6,8
	_ins	$t7,8

	xor	$t0,$t4
	lbu	$t4,2($i0)		# Te4[s3]
	xor	$t1,$t5
	lbu	$t5,2($i1)		# Te4[s0]
	xor	$t2,$t6
	lbu	$t6,2($i2)		# Te4[s1]
	xor	$t3,$t7
	lbu	$t7,2($i3)		# Te4[s2]

	_ins	$t8,24
	lw	$s0,0($key0)
	_ins	$t9,24
	lw	$s1,4($key0)
	_ins	$t10,24
	lw	$s2,8($key0)
	_ins	$t11,24
	lw	$s3,12($key0)

	xor	$t0,$t8
	xor	$t1,$t9
	xor	$t2,$t10
	xor	$t3,$t11

	_ins	$t4,0
	_ins	$t5,0
	_ins	$t6,0
	_ins	$t7,0

	xor	$t0,$t4
	xor	$t1,$t5
	xor	$t2,$t6
	xor	$t3,$t7
#endif
	xor	$s0,$t0
	xor	$s1,$t1
	xor	$s2,$t2
	xor	$s3,$t3

	jr	$ra
.end	_mips_AES_encrypt

.align	5
.globl	AES_encrypt
.ent	AES_encrypt
AES_encrypt:
	.frame	$sp,$FRAMESIZE,$ra
	.mask	$SAVED_REGS_MASK,-$SZREG
	.set	noreorder
___
$code.=<<___ if ($flavour =~ /o32/i);	# o32 PIC-ification
	.cpload	$pf
___
$code.=<<___;
	$PTR_SUB $sp,$FRAMESIZE
	$REG_S	$ra,$FRAMESIZE-1*$SZREG($sp)
	$REG_S	$fp,$FRAMESIZE-2*$SZREG($sp)
	$REG_S	$s11,$FRAMESIZE-3*$SZREG($sp)
	$REG_S	$s10,$FRAMESIZE-4*$SZREG($sp)
	$REG_S	$s9,$FRAMESIZE-5*$SZREG($sp)
	$REG_S	$s8,$FRAMESIZE-6*$SZREG($sp)
	$REG_S	$s7,$FRAMESIZE-7*$SZREG($sp)
	$REG_S	$s6,$FRAMESIZE-8*$SZREG($sp)
	$REG_S	$s5,$FRAMESIZE-9*$SZREG($sp)
	$REG_S	$s4,$FRAMESIZE-10*$SZREG($sp)
___
$code.=<<___ if ($flavour =~ /nubi/i);	# optimize non-nubi prologue
	$REG_S	\$15,$FRAMESIZE-11*$SZREG($sp)
	$REG_S	\$14,$FRAMESIZE-12*$SZREG($sp)
	$REG_S	\$13,$FRAMESIZE-13*$SZREG($sp)
	$REG_S	\$12,$FRAMESIZE-14*$SZREG($sp)
	$REG_S	$gp,$FRAMESIZE-15*$SZREG($sp)
___
$code.=<<___ if ($flavour !~ /o32/i);	# non-o32 PIC-ification
	.cplocal	$Tbl
	.cpsetup	$pf,$zero,AES_encrypt
___
$code.=<<___;
	.set	reorder
	$PTR_LA	$Tbl,AES_Te		# PIC-ified 'load address'

#if defined(_MIPS_ARCH_MIPS32R6) || defined(_MIPS_ARCH_MIPS64R6)
	lw	$s0,0($inp)
	lw	$s1,4($inp)
	lw	$s2,8($inp)
	lw	$s3,12($inp)
#else
	lwl	$s0,0+$MSB($inp)
	lwl	$s1,4+$MSB($inp)
	lwl	$s2,8+$MSB($inp)
	lwl	$s3,12+$MSB($inp)
	lwr	$s0,0+$LSB($inp)
	lwr	$s1,4+$LSB($inp)
	lwr	$s2,8+$LSB($inp)
	lwr	$s3,12+$LSB($inp)
#endif

	bal	_mips_AES_encrypt

#if defined(_MIPS_ARCH_MIPS32R6) || defined(_MIPS_ARCH_MIPS64R6)
	sw	$s0,0($out)
	sw	$s1,4($out)
	sw	$s2,8($out)
	sw	$s3,12($out)
#else
	swr	$s0,0+$LSB($out)
	swr	$s1,4+$LSB($out)
	swr	$s2,8+$LSB($out)
	swr	$s3,12+$LSB($out)
	swl	$s0,0+$MSB($out)
	swl	$s1,4+$MSB($out)
	swl	$s2,8+$MSB($out)
	swl	$s3,12+$MSB($out)
#endif

	.set	noreorder
	$REG_L	$ra,$FRAMESIZE-1*$SZREG($sp)
	$REG_L	$fp,$FRAMESIZE-2*$SZREG($sp)
	$REG_L	$s11,$FRAMESIZE-3*$SZREG($sp)
	$REG_L	$s10,$FRAMESIZE-4*$SZREG($sp)
	$REG_L	$s9,$FRAMESIZE-5*$SZREG($sp)
	$REG_L	$s8,$FRAMESIZE-6*$SZREG($sp)
	$REG_L	$s7,$FRAMESIZE-7*$SZREG($sp)
	$REG_L	$s6,$FRAMESIZE-8*$SZREG($sp)
	$REG_L	$s5,$FRAMESIZE-9*$SZREG($sp)
	$REG_L	$s4,$FRAMESIZE-10*$SZREG($sp)
___
$code.=<<___ if ($flavour =~ /nubi/i);
	$REG_L	\$15,$FRAMESIZE-11*$SZREG($sp)
	$REG_L	\$14,$FRAMESIZE-12*$SZREG($sp)
	$REG_L	\$13,$FRAMESIZE-13*$SZREG($sp)
	$REG_L	\$12,$FRAMESIZE-14*$SZREG($sp)
	$REG_L	$gp,$FRAMESIZE-15*$SZREG($sp)
___
$code.=<<___;
	jr	$ra
	$PTR_ADD $sp,$FRAMESIZE
.end	AES_encrypt
___

$code.=<<___;
.align	5
.ent	_mips_AES_decrypt
_mips_AES_decrypt:
	.frame	$sp,0,$ra
	.set	reorder
	lw	$t0,0($key)
	lw	$t1,4($key)
	lw	$t2,8($key)
	lw	$t3,12($key)
	lw	$cnt,240($key)
	$PTR_ADD $key0,$key,16

	xor	$s0,$t0
	xor	$s1,$t1
	xor	$s2,$t2
	xor	$s3,$t3

	subu	$cnt,1
#if defined(__mips_smartmips)
	ext	$i0,$s3,16,8
.Loop_dec:
	ext	$i1,$s0,16,8
	ext	$i2,$s1,16,8
	ext	$i3,$s2,16,8
	lwxs	$t0,$i0($Tbl)		# Td1[s3>>16]
	ext	$i0,$s2,8,8
	lwxs	$t1,$i1($Tbl)		# Td1[s0>>16]
	ext	$i1,$s3,8,8
	lwxs	$t2,$i2($Tbl)		# Td1[s1>>16]
	ext	$i2,$s0,8,8
	lwxs	$t3,$i3($Tbl)		# Td1[s2>>16]
	ext	$i3,$s1,8,8

	lwxs	$t4,$i0($Tbl)		# Td2[s2>>8]
	ext	$i0,$s1,0,8
	lwxs	$t5,$i1($Tbl)		# Td2[s3>>8]
	ext	$i1,$s2,0,8
	lwxs	$t6,$i2($Tbl)		# Td2[s0>>8]
	ext	$i2,$s3,0,8
	lwxs	$t7,$i3($Tbl)		# Td2[s1>>8]
	ext	$i3,$s0,0,8

	lwxs	$t8,$i0($Tbl)		# Td3[s1]
	ext	$i0,$s0,24,8
	lwxs	$t9,$i1($Tbl)		# Td3[s2]
	ext	$i1,$s1,24,8
	lwxs	$t10,$i2($Tbl)		# Td3[s3]
	ext	$i2,$s2,24,8
	lwxs	$t11,$i3($Tbl)		# Td3[s0]
	ext	$i3,$s3,24,8

	rotr	$t0,$t0,8
	rotr	$t1,$t1,8
	rotr	$t2,$t2,8
	rotr	$t3,$t3,8

	rotr	$t4,$t4,16
	rotr	$t5,$t5,16
	rotr	$t6,$t6,16
	rotr	$t7,$t7,16

	xor	$t0,$t4
	lwxs	$t4,$i0($Tbl)		# Td0[s0>>24]
	xor	$t1,$t5
	lwxs	$t5,$i1($Tbl)		# Td0[s1>>24]
	xor	$t2,$t6
	lwxs	$t6,$i2($Tbl)		# Td0[s2>>24]
	xor	$t3,$t7
	lwxs	$t7,$i3($Tbl)		# Td0[s3>>24]

	rotr	$t8,$t8,24
	lw	$s0,0($key0)
	rotr	$t9,$t9,24
	lw	$s1,4($key0)
	rotr	$t10,$t10,24
	lw	$s2,8($key0)
	rotr	$t11,$t11,24
	lw	$s3,12($key0)

	xor	$t0,$t8
	xor	$t1,$t9
	xor	$t2,$t10
	xor	$t3,$t11

	xor	$t0,$t4
	xor	$t1,$t5
	xor	$t2,$t6
	xor	$t3,$t7

	subu	$cnt,1
	$PTR_ADD $key0,16
	xor	$s0,$t0
	xor	$s1,$t1
	xor	$s2,$t2
	xor	$s3,$t3
	.set	noreorder
	bnez	$cnt,.Loop_dec
	ext	$i0,$s3,16,8

	_xtr	$i0,$s3,16-2
#else
	_xtr	$i0,$s3,16-2
.Loop_dec:
	_xtr	$i1,$s0,16-2
	_xtr	$i2,$s1,16-2
	_xtr	$i3,$s2,16-2
	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
#if defined(_MIPS_ARCH_MIPS32R2) || defined(_MIPS_ARCH_MIPS64R2)
	lw	$t0,0($i0)		# Td1[s3>>16]
	_xtr	$i0,$s2,8-2
	lw	$t1,0($i1)		# Td1[s0>>16]
	_xtr	$i1,$s3,8-2
	lw	$t2,0($i2)		# Td1[s1>>16]
	_xtr	$i2,$s0,8-2
	lw	$t3,0($i3)		# Td1[s2>>16]
	_xtr	$i3,$s1,8-2
#else
	lwl	$t0,3($i0)		# Td1[s3>>16]
	lwl	$t1,3($i1)		# Td1[s0>>16]
	lwl	$t2,3($i2)		# Td1[s1>>16]
	lwl	$t3,3($i3)		# Td1[s2>>16]
	lwr	$t0,2($i0)		# Td1[s3>>16]
	_xtr	$i0,$s2,8-2
	lwr	$t1,2($i1)		# Td1[s0>>16]
	_xtr	$i1,$s3,8-2
	lwr	$t2,2($i2)		# Td1[s1>>16]
	_xtr	$i2,$s0,8-2
	lwr	$t3,2($i3)		# Td1[s2>>16]
	_xtr	$i3,$s1,8-2
#endif

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
#if defined(_MIPS_ARCH_MIPS32R2) || defined(_MIPS_ARCH_MIPS64R2)
	rotr	$t0,$t0,8
	rotr	$t1,$t1,8
	rotr	$t2,$t2,8
	rotr	$t3,$t3,8
# if defined(_MIPSEL)
	lw	$t4,0($i0)		# Td2[s2>>8]
	_xtr	$i0,$s1,0-2
	lw	$t5,0($i1)		# Td2[s3>>8]
	_xtr	$i1,$s2,0-2
	lw	$t6,0($i2)		# Td2[s0>>8]
	_xtr	$i2,$s3,0-2
	lw	$t7,0($i3)		# Td2[s1>>8]
	_xtr	$i3,$s0,0-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lw	$t8,0($i0)		# Td3[s1]
	$PTR_INS $i0,$s0,2,8
	lw	$t9,0($i1)		# Td3[s2]
	$PTR_INS $i1,$s1,2,8
	lw	$t10,0($i2)		# Td3[s3]
	$PTR_INS $i2,$s2,2,8
	lw	$t11,0($i3)		# Td3[s0]
	$PTR_INS $i3,$s3,2,8
#else
	lw	$t4,0($i0)		# Td2[s2>>8]
	$PTR_INS $i0,$s1,2,8
	lw	$t5,0($i1)		# Td2[s3>>8]
	$PTR_INS $i1,$s2,2,8
	lw	$t6,0($i2)		# Td2[s0>>8]
	$PTR_INS $i2,$s3,2,8
	lw	$t7,0($i3)		# Td2[s1>>8]
	$PTR_INS $i3,$s0,2,8

	lw	$t8,0($i0)		# Td3[s1]
	_xtr	$i0,$s0,24-2
	lw	$t9,0($i1)		# Td3[s2]
	_xtr	$i1,$s1,24-2
	lw	$t10,0($i2)		# Td3[s3]
	_xtr	$i2,$s2,24-2
	lw	$t11,0($i3)		# Td3[s0]
	_xtr	$i3,$s3,24-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
#endif
	rotr	$t4,$t4,16
	rotr	$t5,$t5,16
	rotr	$t6,$t6,16
	rotr	$t7,$t7,16

	rotr	$t8,$t8,24
	rotr	$t9,$t9,24
	rotr	$t10,$t10,24
	rotr	$t11,$t11,24
#else
	lwl	$t4,2($i0)		# Td2[s2>>8]
	lwl	$t5,2($i1)		# Td2[s3>>8]
	lwl	$t6,2($i2)		# Td2[s0>>8]
	lwl	$t7,2($i3)		# Td2[s1>>8]
	lwr	$t4,1($i0)		# Td2[s2>>8]
	_xtr	$i0,$s1,0-2
	lwr	$t5,1($i1)		# Td2[s3>>8]
	_xtr	$i1,$s2,0-2
	lwr	$t6,1($i2)		# Td2[s0>>8]
	_xtr	$i2,$s3,0-2
	lwr	$t7,1($i3)		# Td2[s1>>8]
	_xtr	$i3,$s0,0-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lwl	$t8,1($i0)		# Td3[s1]
	lwl	$t9,1($i1)		# Td3[s2]
	lwl	$t10,1($i2)		# Td3[s3]
	lwl	$t11,1($i3)		# Td3[s0]
	lwr	$t8,0($i0)		# Td3[s1]
	_xtr	$i0,$s0,24-2
	lwr	$t9,0($i1)		# Td3[s2]
	_xtr	$i1,$s1,24-2
	lwr	$t10,0($i2)		# Td3[s3]
	_xtr	$i2,$s2,24-2
	lwr	$t11,0($i3)		# Td3[s0]
	_xtr	$i3,$s3,24-2

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
#endif

	xor	$t0,$t4
	lw	$t4,0($i0)		# Td0[s0>>24]
	xor	$t1,$t5
	lw	$t5,0($i1)		# Td0[s1>>24]
	xor	$t2,$t6
	lw	$t6,0($i2)		# Td0[s2>>24]
	xor	$t3,$t7
	lw	$t7,0($i3)		# Td0[s3>>24]

	xor	$t0,$t8
	lw	$s0,0($key0)
	xor	$t1,$t9
	lw	$s1,4($key0)
	xor	$t2,$t10
	lw	$s2,8($key0)
	xor	$t3,$t11
	lw	$s3,12($key0)

	xor	$t0,$t4
	xor	$t1,$t5
	xor	$t2,$t6
	xor	$t3,$t7

	subu	$cnt,1
	$PTR_ADD $key0,16
	xor	$s0,$t0
	xor	$s1,$t1
	xor	$s2,$t2
	xor	$s3,$t3
	.set	noreorder
	bnez	$cnt,.Loop_dec
	_xtr	$i0,$s3,16-2
#endif

	.set	reorder
	lw	$t4,1024($Tbl)		# prefetch Td4
	_xtr	$i0,$s3,16
	lw	$t5,1024+32($Tbl)
	_xtr	$i1,$s0,16
	lw	$t6,1024+64($Tbl)
	_xtr	$i2,$s1,16
	lw	$t7,1024+96($Tbl)
	_xtr	$i3,$s2,16
	lw	$t8,1024+128($Tbl)
	and	$i0,0xFF
	lw	$t9,1024+160($Tbl)
	and	$i1,0xFF
	lw	$t10,1024+192($Tbl)
	and	$i2,0xFF
	lw	$t11,1024+224($Tbl)
	and	$i3,0xFF

	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lbu	$t0,1024($i0)		# Td4[s3>>16]
	_xtr	$i0,$s2,8
	lbu	$t1,1024($i1)		# Td4[s0>>16]
	_xtr	$i1,$s3,8
	lbu	$t2,1024($i2)		# Td4[s1>>16]
	_xtr	$i2,$s0,8
	lbu	$t3,1024($i3)		# Td4[s2>>16]
	_xtr	$i3,$s1,8

	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	and	$i3,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
#if defined(_MIPS_ARCH_MIPS32R2) || defined(_MIPS_ARCH_MIPS64R2)
# if defined(_MIPSEL)
	lbu	$t4,1024($i0)		# Td4[s2>>8]
	$PTR_INS $i0,$s0,0,8
	lbu	$t5,1024($i1)		# Td4[s3>>8]
	$PTR_INS $i1,$s1,0,8
	lbu	$t6,1024($i2)		# Td4[s0>>8]
	$PTR_INS $i2,$s2,0,8
	lbu	$t7,1024($i3)		# Td4[s1>>8]
	$PTR_INS $i3,$s3,0,8

	lbu	$t8,1024($i0)		# Td4[s0>>24]
	_xtr	$i0,$s1,0
	lbu	$t9,1024($i1)		# Td4[s1>>24]
	_xtr	$i1,$s2,0
	lbu	$t10,1024($i2)		# Td4[s2>>24]
	_xtr	$i2,$s3,0
	lbu	$t11,1024($i3)		# Td4[s3>>24]
	_xtr	$i3,$s0,0

	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
# else
	lbu	$t4,1024($i0)		# Td4[s2>>8]
	_xtr	$i0,$s0,24
	lbu	$t5,1024($i1)		# Td4[s3>>8]
	_xtr	$i1,$s1,24
	lbu	$t6,1024($i2)		# Td4[s0>>8]
	_xtr	$i2,$s2,24
	lbu	$t7,1024($i3)		# Td4[s1>>8]
	_xtr	$i3,$s3,24

	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lbu	$t8,1024($i0)		# Td4[s0>>24]
	$PTR_INS $i0,$s1,0,8
	lbu	$t9,1024($i1)		# Td4[s1>>24]
	$PTR_INS $i1,$s2,0,8
	lbu	$t10,1024($i2)		# Td4[s2>>24]
	$PTR_INS $i2,$s3,0,8
	lbu	$t11,1024($i3)		# Td4[s3>>24]
	$PTR_INS $i3,$s0,0,8
# endif
	_ins	$t0,16
	_ins	$t1,16
	_ins	$t2,16
	_ins	$t3,16

	_ins2	$t0,$t4,8
	lbu	$t4,1024($i0)		# Td4[s1]
	_ins2	$t1,$t5,8
	lbu	$t5,1024($i1)		# Td4[s2]
	_ins2	$t2,$t6,8
	lbu	$t6,1024($i2)		# Td4[s3]
	_ins2	$t3,$t7,8
	lbu	$t7,1024($i3)		# Td4[s0]

	_ins2	$t0,$t8,24
	lw	$s0,0($key0)
	_ins2	$t1,$t9,24
	lw	$s1,4($key0)
	_ins2	$t2,$t10,24
	lw	$s2,8($key0)
	_ins2	$t3,$t11,24
	lw	$s3,12($key0)

	_ins2	$t0,$t4,0
	_ins2	$t1,$t5,0
	_ins2	$t2,$t6,0
	_ins2	$t3,$t7,0
#else
	lbu	$t4,1024($i0)		# Td4[s2>>8]
	_xtr	$i0,$s0,24
	lbu	$t5,1024($i1)		# Td4[s3>>8]
	_xtr	$i1,$s1,24
	lbu	$t6,1024($i2)		# Td4[s0>>8]
	_xtr	$i2,$s2,24
	lbu	$t7,1024($i3)		# Td4[s1>>8]
	_xtr	$i3,$s3,24

	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lbu	$t8,1024($i0)		# Td4[s0>>24]
	_xtr	$i0,$s1,0
	lbu	$t9,1024($i1)		# Td4[s1>>24]
	_xtr	$i1,$s2,0
	lbu	$t10,1024($i2)		# Td4[s2>>24]
	_xtr	$i2,$s3,0
	lbu	$t11,1024($i3)		# Td4[s3>>24]
	_xtr	$i3,$s0,0

	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl

	_ins	$t0,16
	_ins	$t1,16
	_ins	$t2,16
	_ins	$t3,16

	_ins	$t4,8
	_ins	$t5,8
	_ins	$t6,8
	_ins	$t7,8

	xor	$t0,$t4
	lbu	$t4,1024($i0)		# Td4[s1]
	xor	$t1,$t5
	lbu	$t5,1024($i1)		# Td4[s2]
	xor	$t2,$t6
	lbu	$t6,1024($i2)		# Td4[s3]
	xor	$t3,$t7
	lbu	$t7,1024($i3)		# Td4[s0]

	_ins	$t8,24
	lw	$s0,0($key0)
	_ins	$t9,24
	lw	$s1,4($key0)
	_ins	$t10,24
	lw	$s2,8($key0)
	_ins	$t11,24
	lw	$s3,12($key0)

	xor	$t0,$t8
	xor	$t1,$t9
	xor	$t2,$t10
	xor	$t3,$t11

	_ins	$t4,0
	_ins	$t5,0
	_ins	$t6,0
	_ins	$t7,0

	xor	$t0,$t4
	xor	$t1,$t5
	xor	$t2,$t6
	xor	$t3,$t7
#endif

	xor	$s0,$t0
	xor	$s1,$t1
	xor	$s2,$t2
	xor	$s3,$t3

	jr	$ra
.end	_mips_AES_decrypt

.align	5
.globl	AES_decrypt
.ent	AES_decrypt
AES_decrypt:
	.frame	$sp,$FRAMESIZE,$ra
	.mask	$SAVED_REGS_MASK,-$SZREG
	.set	noreorder
___
$code.=<<___ if ($flavour =~ /o32/i);	# o32 PIC-ification
	.cpload	$pf
___
$code.=<<___;
	$PTR_SUB $sp,$FRAMESIZE
	$REG_S	$ra,$FRAMESIZE-1*$SZREG($sp)
	$REG_S	$fp,$FRAMESIZE-2*$SZREG($sp)
	$REG_S	$s11,$FRAMESIZE-3*$SZREG($sp)
	$REG_S	$s10,$FRAMESIZE-4*$SZREG($sp)
	$REG_S	$s9,$FRAMESIZE-5*$SZREG($sp)
	$REG_S	$s8,$FRAMESIZE-6*$SZREG($sp)
	$REG_S	$s7,$FRAMESIZE-7*$SZREG($sp)
	$REG_S	$s6,$FRAMESIZE-8*$SZREG($sp)
	$REG_S	$s5,$FRAMESIZE-9*$SZREG($sp)
	$REG_S	$s4,$FRAMESIZE-10*$SZREG($sp)
___
$code.=<<___ if ($flavour =~ /nubi/i);	# optimize non-nubi prologue
	$REG_S	\$15,$FRAMESIZE-11*$SZREG($sp)
	$REG_S	\$14,$FRAMESIZE-12*$SZREG($sp)
	$REG_S	\$13,$FRAMESIZE-13*$SZREG($sp)
	$REG_S	\$12,$FRAMESIZE-14*$SZREG($sp)
	$REG_S	$gp,$FRAMESIZE-15*$SZREG($sp)
___
$code.=<<___ if ($flavour !~ /o32/i);	# non-o32 PIC-ification
	.cplocal	$Tbl
	.cpsetup	$pf,$zero,AES_decrypt
___
$code.=<<___;
	.set	reorder
	$PTR_LA	$Tbl,AES_Td		# PIC-ified 'load address'

#if defined(_MIPS_ARCH_MIPS32R6) || defined(_MIPS_ARCH_MIPS64R6)
	lw	$s0,0($inp)
	lw	$s1,4($inp)
	lw	$s2,8($inp)
	lw	$s3,12($inp)
#else
	lwl	$s0,0+$MSB($inp)
	lwl	$s1,4+$MSB($inp)
	lwl	$s2,8+$MSB($inp)
	lwl	$s3,12+$MSB($inp)
	lwr	$s0,0+$LSB($inp)
	lwr	$s1,4+$LSB($inp)
	lwr	$s2,8+$LSB($inp)
	lwr	$s3,12+$LSB($inp)
#endif

	bal	_mips_AES_decrypt

#if defined(_MIPS_ARCH_MIPS32R6) || defined(_MIPS_ARCH_MIPS64R6)
	sw	$s0,0($out)
	sw	$s1,4($out)
	sw	$s2,8($out)
	sw	$s3,12($out)
#else
	swr	$s0,0+$LSB($out)
	swr	$s1,4+$LSB($out)
	swr	$s2,8+$LSB($out)
	swr	$s3,12+$LSB($out)
	swl	$s0,0+$MSB($out)
	swl	$s1,4+$MSB($out)
	swl	$s2,8+$MSB($out)
	swl	$s3,12+$MSB($out)
#endif

	.set	noreorder
	$REG_L	$ra,$FRAMESIZE-1*$SZREG($sp)
	$REG_L	$fp,$FRAMESIZE-2*$SZREG($sp)
	$REG_L	$s11,$FRAMESIZE-3*$SZREG($sp)
	$REG_L	$s10,$FRAMESIZE-4*$SZREG($sp)
	$REG_L	$s9,$FRAMESIZE-5*$SZREG($sp)
	$REG_L	$s8,$FRAMESIZE-6*$SZREG($sp)
	$REG_L	$s7,$FRAMESIZE-7*$SZREG($sp)
	$REG_L	$s6,$FRAMESIZE-8*$SZREG($sp)
	$REG_L	$s5,$FRAMESIZE-9*$SZREG($sp)
	$REG_L	$s4,$FRAMESIZE-10*$SZREG($sp)
___
$code.=<<___ if ($flavour =~ /nubi/i);
	$REG_L	\$15,$FRAMESIZE-11*$SZREG($sp)
	$REG_L	\$14,$FRAMESIZE-12*$SZREG($sp)
	$REG_L	\$13,$FRAMESIZE-13*$SZREG($sp)
	$REG_L	\$12,$FRAMESIZE-14*$SZREG($sp)
	$REG_L	$gp,$FRAMESIZE-15*$SZREG($sp)
___
$code.=<<___;
	jr	$ra
	$PTR_ADD $sp,$FRAMESIZE
.end	AES_decrypt
___
}}}

{{{
my $FRAMESIZE=8*$SZREG;
my $SAVED_REGS_MASK = ($flavour =~ /nubi/i) ? "0xFF" : "0xFF";

my ($inp,$bits,$key,$Tbl)=($a0,$a1,$a2,$a3);
my ($rk0,$rk1,$rk2,$rk3,$rk4,$rk5,$rk6,$rk7)=($a4,$a5,$a6,$a7,$s0,$s1,$s2,$s3);
my ($i0,$i1,$i2,$i3)=($at,$t0,$t1,$t2);
my ($rcon,$cnt)=($gp,$fp);

$code.=<<___;
.align	5
.ent	_mips_AES_set_encrypt_key
_mips_AES_set_encrypt_key:
	.frame	$sp,0,$ra
	.set	noreorder
	beqz	$inp,.Lekey_done
	li	$t0,-1
	beqz	$key,.Lekey_done
	$PTR_ADD $rcon,$Tbl,256

	.set	reorder
#if defined(_MIPS_ARCH_MIPS32R6) || defined(_MIPS_ARCH_MIPS64R6)
	lw	$rk0,0($inp)		# load 128 bits
	lw	$rk1,4($inp)
	lw	$rk2,8($inp)
	lw	$rk3,12($inp)
#else
	lwl	$rk0,0+$MSB($inp)	# load 128 bits
	lwl	$rk1,4+$MSB($inp)
	lwl	$rk2,8+$MSB($inp)
	lwl	$rk3,12+$MSB($inp)
	lwr	$rk0,0+$LSB($inp)
	lwr	$rk1,4+$LSB($inp)
	lwr	$rk2,8+$LSB($inp)
	lwr	$rk3,12+$LSB($inp)
#endif
	li	$at,128
	.set	noreorder
	beq	$bits,$at,.L128bits
	li	$cnt,10

	.set	reorder
#if defined(_MIPS_ARCH_MIPS32R6) || defined(_MIPS_ARCH_MIPS64R6)
	lw	$rk4,16($inp)		# load 192 bits
	lw	$rk5,20($inp)
#else
	lwl	$rk4,16+$MSB($inp)	# load 192 bits
	lwl	$rk5,20+$MSB($inp)
	lwr	$rk4,16+$LSB($inp)
	lwr	$rk5,20+$LSB($inp)
#endif
	li	$at,192
	.set	noreorder
	beq	$bits,$at,.L192bits
	li	$cnt,8

	.set	reorder
#if defined(_MIPS_ARCH_MIPS32R6) || defined(_MIPS_ARCH_MIPS64R6)
	lw	$rk6,24($inp)		# load 256 bits
	lw	$rk7,28($inp)
#else
	lwl	$rk6,24+$MSB($inp)	# load 256 bits
	lwl	$rk7,28+$MSB($inp)
	lwr	$rk6,24+$LSB($inp)
	lwr	$rk7,28+$LSB($inp)
#endif
	li	$at,256
	.set	noreorder
	beq	$bits,$at,.L256bits
	li	$cnt,7

	b	.Lekey_done
	li	$t0,-2

.align	4
.L128bits:
	.set	reorder
	srl	$i0,$rk3,16
	srl	$i1,$rk3,8
	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,$rk3,0xFF
	srl	$i3,$rk3,24
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lbu	$i0,0($i0)
	lbu	$i1,0($i1)
	lbu	$i2,0($i2)
	lbu	$i3,0($i3)

	sw	$rk0,0($key)
	sw	$rk1,4($key)
	sw	$rk2,8($key)
	sw	$rk3,12($key)
	subu	$cnt,1
	$PTR_ADD $key,16

	_bias	$i0,24
	_bias	$i1,16
	_bias	$i2,8
	_bias	$i3,0

	xor	$rk0,$i0
	lw	$i0,0($rcon)
	xor	$rk0,$i1
	xor	$rk0,$i2
	xor	$rk0,$i3
	xor	$rk0,$i0

	xor	$rk1,$rk0
	xor	$rk2,$rk1
	xor	$rk3,$rk2

	.set	noreorder
	bnez	$cnt,.L128bits
	$PTR_ADD $rcon,4

	sw	$rk0,0($key)
	sw	$rk1,4($key)
	sw	$rk2,8($key)
	li	$cnt,10
	sw	$rk3,12($key)
	li	$t0,0
	sw	$cnt,80($key)
	b	.Lekey_done
	$PTR_SUB $key,10*16

.align	4
.L192bits:
	.set	reorder
	srl	$i0,$rk5,16
	srl	$i1,$rk5,8
	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,$rk5,0xFF
	srl	$i3,$rk5,24
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lbu	$i0,0($i0)
	lbu	$i1,0($i1)
	lbu	$i2,0($i2)
	lbu	$i3,0($i3)

	sw	$rk0,0($key)
	sw	$rk1,4($key)
	sw	$rk2,8($key)
	sw	$rk3,12($key)
	sw	$rk4,16($key)
	sw	$rk5,20($key)
	subu	$cnt,1
	$PTR_ADD $key,24

	_bias	$i0,24
	_bias	$i1,16
	_bias	$i2,8
	_bias	$i3,0

	xor	$rk0,$i0
	lw	$i0,0($rcon)
	xor	$rk0,$i1
	xor	$rk0,$i2
	xor	$rk0,$i3
	xor	$rk0,$i0

	xor	$rk1,$rk0
	xor	$rk2,$rk1
	xor	$rk3,$rk2
	xor	$rk4,$rk3
	xor	$rk5,$rk4

	.set	noreorder
	bnez	$cnt,.L192bits
	$PTR_ADD $rcon,4

	sw	$rk0,0($key)
	sw	$rk1,4($key)
	sw	$rk2,8($key)
	li	$cnt,12
	sw	$rk3,12($key)
	li	$t0,0
	sw	$cnt,48($key)
	b	.Lekey_done
	$PTR_SUB $key,12*16

.align	4
.L256bits:
	.set	reorder
	srl	$i0,$rk7,16
	srl	$i1,$rk7,8
	and	$i0,0xFF
	and	$i1,0xFF
	and	$i2,$rk7,0xFF
	srl	$i3,$rk7,24
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lbu	$i0,0($i0)
	lbu	$i1,0($i1)
	lbu	$i2,0($i2)
	lbu	$i3,0($i3)

	sw	$rk0,0($key)
	sw	$rk1,4($key)
	sw	$rk2,8($key)
	sw	$rk3,12($key)
	sw	$rk4,16($key)
	sw	$rk5,20($key)
	sw	$rk6,24($key)
	sw	$rk7,28($key)
	subu	$cnt,1

	_bias	$i0,24
	_bias	$i1,16
	_bias	$i2,8
	_bias	$i3,0

	xor	$rk0,$i0
	lw	$i0,0($rcon)
	xor	$rk0,$i1
	xor	$rk0,$i2
	xor	$rk0,$i3
	xor	$rk0,$i0

	xor	$rk1,$rk0
	xor	$rk2,$rk1
	xor	$rk3,$rk2
	beqz	$cnt,.L256bits_done

	srl	$i0,$rk3,24
	srl	$i1,$rk3,16
	srl	$i2,$rk3,8
	and	$i3,$rk3,0xFF
	and	$i1,0xFF
	and	$i2,0xFF
	$PTR_ADD $i0,$Tbl
	$PTR_ADD $i1,$Tbl
	$PTR_ADD $i2,$Tbl
	$PTR_ADD $i3,$Tbl
	lbu	$i0,0($i0)
	lbu	$i1,0($i1)
	lbu	$i2,0($i2)
	lbu	$i3,0($i3)
	sll	$i0,24
	sll	$i1,16
	sll	$i2,8

	xor	$rk4,$i0
	xor	$rk4,$i1
	xor	$rk4,$i2
	xor	$rk4,$i3

	xor	$rk5,$rk4
	xor	$rk6,$rk5
	xor	$rk7,$rk6

	$PTR_ADD $key,32
	.set	noreorder
	b	.L256bits
	$PTR_ADD $rcon,4

.L256bits_done:
	sw	$rk0,32($key)
	sw	$rk1,36($key)
	sw	$rk2,40($key)
	li	$cnt,14
	sw	$rk3,44($key)
	li	$t0,0
	sw	$cnt,48($key)
	$PTR_SUB $key,12*16

.Lekey_done:
	jr	$ra
	nop
.end	_mips_AES_set_encrypt_key

.globl	AES_set_encrypt_key
.ent	AES_set_encrypt_key
AES_set_encrypt_key:
	.frame	$sp,$FRAMESIZE,$ra
	.mask	$SAVED_REGS_MASK,-$SZREG
	.set	noreorder
___
$code.=<<___ if ($flavour =~ /o32/i);	# o32 PIC-ification
	.cpload	$pf
___
$code.=<<___;
	$PTR_SUB $sp,$FRAMESIZE
	$REG_S	$ra,$FRAMESIZE-1*$SZREG($sp)
	$REG_S	$fp,$FRAMESIZE-2*$SZREG($sp)
___
$code.=<<___ if ($flavour =~ /nubi/i);	# optimize non-nubi prologue
	$REG_S	$s3,$FRAMESIZE-3*$SZREG($sp)
	$REG_S	$s2,$FRAMESIZE-4*$SZREG($sp)
	$REG_S	$s1,$FRAMESIZE-5*$SZREG($sp)
	$REG_S	$s0,$FRAMESIZE-6*$SZREG($sp)
	$REG_S	$gp,$FRAMESIZE-7*$SZREG($sp)
___
$code.=<<___ if ($flavour !~ /o32/i);	# non-o32 PIC-ification
	.cplocal	$Tbl
	.cpsetup	$pf,$zero,AES_set_encrypt_key
___
$code.=<<___;
	.set	reorder
	$PTR_LA	$Tbl,AES_Te4		# PIC-ified 'load address'

	bal	_mips_AES_set_encrypt_key

	.set	noreorder
	move	$a0,$t0
	$REG_L	$ra,$FRAMESIZE-1*$SZREG($sp)
	$REG_L	$fp,$FRAMESIZE-2*$SZREG($sp)
___
$code.=<<___ if ($flavour =~ /nubi/i);
	$REG_L	$s3,$FRAMESIZE-11*$SZREG($sp)
	$REG_L	$s2,$FRAMESIZE-12*$SZREG($sp)
	$REG_L	$s1,$FRAMESIZE-13*$SZREG($sp)
	$REG_L	$s0,$FRAMESIZE-14*$SZREG($sp)
	$REG_L	$gp,$FRAMESIZE-15*$SZREG($sp)
___
$code.=<<___;
	jr	$ra
	$PTR_ADD $sp,$FRAMESIZE
.end	AES_set_encrypt_key
___

my ($head,$tail)=($inp,$bits);
my ($tp1,$tp2,$tp4,$tp8,$tp9,$tpb,$tpd,$tpe)=($a4,$a5,$a6,$a7,$s0,$s1,$s2,$s3);
my ($m,$x80808080,$x7f7f7f7f,$x1b1b1b1b)=($at,$t0,$t1,$t2);
$code.=<<___;
.align	5
.globl	AES_set_decrypt_key
.ent	AES_set_decrypt_key
AES_set_decrypt_key:
	.frame	$sp,$FRAMESIZE,$ra
	.mask	$SAVED_REGS_MASK,-$SZREG
	.set	noreorder
___
$code.=<<___ if ($flavour =~ /o32/i);	# o32 PIC-ification
	.cpload	$pf
___
$code.=<<___;
	$PTR_SUB $sp,$FRAMESIZE
	$REG_S	$ra,$FRAMESIZE-1*$SZREG($sp)
	$REG_S	$fp,$FRAMESIZE-2*$SZREG($sp)
___
$code.=<<___ if ($flavour =~ /nubi/i);	# optimize non-nubi prologue
	$REG_S	$s3,$FRAMESIZE-3*$SZREG($sp)
	$REG_S	$s2,$FRAMESIZE-4*$SZREG($sp)
	$REG_S	$s1,$FRAMESIZE-5*$SZREG($sp)
	$REG_S	$s0,$FRAMESIZE-6*$SZREG($sp)
	$REG_S	$gp,$FRAMESIZE-7*$SZREG($sp)
___
$code.=<<___ if ($flavour !~ /o32/i);	# non-o32 PIC-ification
	.cplocal	$Tbl
	.cpsetup	$pf,$zero,AES_set_decrypt_key
___
$code.=<<___;
	.set	reorder
	$PTR_LA	$Tbl,AES_Te4		# PIC-ified 'load address'

	bal	_mips_AES_set_encrypt_key

	bltz	$t0,.Ldkey_done

	sll	$at,$cnt,4
	$PTR_ADD $head,$key,0
	$PTR_ADD $tail,$key,$at
.align	4
.Lswap:
	lw	$rk0,0($head)
	lw	$rk1,4($head)
	lw	$rk2,8($head)
	lw	$rk3,12($head)
	lw	$rk4,0($tail)
	lw	$rk5,4($tail)
	lw	$rk6,8($tail)
	lw	$rk7,12($tail)
	sw	$rk0,0($tail)
	sw	$rk1,4($tail)
	sw	$rk2,8($tail)
	sw	$rk3,12($tail)
	$PTR_ADD $head,16
	$PTR_SUB $tail,16
	sw	$rk4,-16($head)
	sw	$rk5,-12($head)
	sw	$rk6,-8($head)
	sw	$rk7,-4($head)
	bne	$head,$tail,.Lswap

	lw	$tp1,16($key)		# modulo-scheduled
	lui	$x80808080,0xFF
	subu	$cnt,1
	or	$x80808080,0xFF
	sll	$cnt,2
	$PTR_ADD $key,16
	lui	$x1b1b1b1b,0xFF
	nor	$x7f7f7f7f,$zero,$x80808080
	or	$x1b1b1b1b,0xFF
.align	4
.Lmix:
	and	$m,$tp1,$x80808080
	and	$tp2,$tp1,$x7f7f7f7f
	srl	$tp4,$m,7
	addu	$tp2,$tp2		# tp2<<1
	subu	$m,$tp4
	and	$m,$x1b1b1b1b
	xor	$tp2,$m

	and	$m,$tp2,$x80808080
	and	$tp4,$tp2,$x7f7f7f7f
	srl	$tp8,$m,7
	addu	$tp4,$tp4		# tp4<<1
	subu	$m,$tp8
	and	$m,$x1b1b1b1b
	xor	$tp4,$m

	and	$m,$tp4,$x80808080
	and	$tp8,$tp4,$x7f7f7f7f
	srl	$tp9,$m,7
	addu	$tp8,$tp8		# tp8<<1
	subu	$m,$tp9
	and	$m,$x1b1b1b1b
	xor	$tp8,$m

	xor	$tp9,$tp8,$tp1
	xor	$tpe,$tp8,$tp4
	xor	$tpb,$tp9,$tp2
	xor	$tpd,$tp9,$tp4

#if defined(_MIPS_ARCH_MIPS32R2) || defined(_MIPS_ARCH_MIPS64R2)
	rotr	$tp1,$tpd,16
	 xor	$tpe,$tp2
	rotr	$tp2,$tp9,8
	xor	$tpe,$tp1
	rotr	$tp4,$tpb,24
	xor	$tpe,$tp2
	lw	$tp1,4($key)		# modulo-scheduled
	xor	$tpe,$tp4
#else
	_ror	$tp1,$tpd,16
	 xor	$tpe,$tp2
	_ror	$tp2,$tpd,-16
	xor	$tpe,$tp1
	_ror	$tp1,$tp9,8
	xor	$tpe,$tp2
	_ror	$tp2,$tp9,-24
	xor	$tpe,$tp1
	_ror	$tp1,$tpb,24
	xor	$tpe,$tp2
	_ror	$tp2,$tpb,-8
	xor	$tpe,$tp1
	lw	$tp1,4($key)		# modulo-scheduled
	xor	$tpe,$tp2
#endif
	subu	$cnt,1
	sw	$tpe,0($key)
	$PTR_ADD $key,4
	bnez	$cnt,.Lmix

	li	$t0,0
.Ldkey_done:
	.set	noreorder
	move	$a0,$t0
	$REG_L	$ra,$FRAMESIZE-1*$SZREG($sp)
	$REG_L	$fp,$FRAMESIZE-2*$SZREG($sp)
___
$code.=<<___ if ($flavour =~ /nubi/i);
	$REG_L	$s3,$FRAMESIZE-11*$SZREG($sp)
	$REG_L	$s2,$FRAMESIZE-12*$SZREG($sp)
	$REG_L	$s1,$FRAMESIZE-13*$SZREG($sp)
	$REG_L	$s0,$FRAMESIZE-14*$SZREG($sp)
	$REG_L	$gp,$FRAMESIZE-15*$SZREG($sp)
___
$code.=<<___;
	jr	$ra
	$PTR_ADD $sp,$FRAMESIZE
.end	AES_set_decrypt_key
___
}}}

######################################################################
# Tables are kept in endian-neutral manner
$code.=<<___;
.rdata
.align	10
AES_Te:
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF	# Te0
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF

AES_Td:
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF	# Td0
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF

.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF	# Td4
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF

AES_Te4:
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF	# Te4
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF
.byte	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF

.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF	# rcon
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,	0xFF,0xFF,0xFF,0xFF
___

foreach (split("\n",$code)) {
	s/\`([^\`]*)\`/eval $1/ge;

	# made-up _instructions, _xtr, _ins, _ror and _bias, cope
	# with byte order dependencies...
	if (/^\s+_/) {
	    s/(_[a-z]+\s+)(\$[0-9]+),([^,]+)(#.*)*$/$1$2,$2,$3/;

	    s/_xtr\s+(\$[0-9]+),(\$[0-9]+),([0-9]+(\-2)*)/
		sprintf("srl\t$1,$2,%d",$big_endian ?	eval($3)
					:		eval("24-$3"))/e or
	    s/_ins\s+(\$[0-9]+),(\$[0-9]+),([0-9]+)/
		sprintf("sll\t$1,$2,%d",$big_endian ?	eval($3)
					:		eval("24-$3"))/e or
	    s/_ins2\s+(\$[0-9]+),(\$[0-9]+),([0-9]+)/
		sprintf("ins\t$1,$2,%d,8",$big_endian ?	eval($3)
					:		eval("24-$3"))/e or
	    s/_ror\s+(\$[0-9]+),(\$[0-9]+),(\-?[0-9]+)/
		sprintf("srl\t$1,$2,%d",$big_endian ?	eval($3)
					:		eval("$3*-1"))/e or
	    s/_bias\s+(\$[0-9]+),(\$[0-9]+),([0-9]+)/
		sprintf("sll\t$1,$2,%d",$big_endian ?	eval($3)
					:		eval("($3-16)&31"))/e;

	    s/srl\s+(\$[0-9]+),(\$[0-9]+),\-([0-9]+)/
		sprintf("sll\t$1,$2,$3")/e				or
	    s/srl\s+(\$[0-9]+),(\$[0-9]+),0/
		sprintf("and\t$1,$2,0xFF")/e				or
	    s/(sll\s+\$[0-9]+,\$[0-9]+,0)/#$1/;
	}

	# convert lwl/lwr and swr/swl to little-endian order
	if (!$big_endian && /^\s+[sl]w[lr]\s+/) {
	    s/([sl]wl.*)([0-9]+)\((\$[0-9]+)\)/
		sprintf("$1%d($3)",eval("$2-$2%4+($2%4-1)&3"))/e	or
	    s/([sl]wr.*)([0-9]+)\((\$[0-9]+)\)/
		sprintf("$1%d($3)",eval("$2-$2%4+($2%4+1)&3"))/e;
	}

	if (!$big_endian) {
	    s/(rotr\s+\$[0-9]+,\$[0-9]+),([0-9]+)/sprintf("$1,%d",32-$2)/e;
	    s/(ext\s+\$[0-9]+,\$[0-9]+),([0-9]+),8/sprintf("$1,%d,8",24-$2)/e;
	}

	print $_,"\n";
}

close STDOUT;
