.text	
.type	_x86_64_AES_encrypt,@function
.align	16
_x86_64_AES_encrypt:
	xorl	0(%r15),%eax
	xorl	4(%r15),%ebx
	xorl	8(%r15),%ecx
	xorl	12(%r15),%edx

	movl	240(%r15),%r13d
	subl	$1,%r13d
	jmp	.Lenc_loop
.align	16
.Lenc_loop:

	movzbl	%al,%esi
	movzbl	%bl,%edi
	movzbl	%cl,%ebp
	movl	0(%r14,%rsi,8),%r10d
	movl	0(%r14,%rdi,8),%r11d
	movl	0(%r14,%rbp,8),%r12d

	movzbl	%bh,%esi
	movzbl	%ch,%edi
	movzbl	%dl,%ebp
	xorl	3(%r14,%rsi,8),%r10d
	xorl	3(%r14,%rdi,8),%r11d
	movl	0(%r14,%rbp,8),%r8d

	movzbl	%dh,%esi
	shrl	$16,%ecx
	movzbl	%ah,%ebp
	xorl	3(%r14,%rsi,8),%r12d
	shrl	$16,%edx
	xorl	3(%r14,%rbp,8),%r8d

	shrl	$16,%ebx
	leaq	16(%r15),%r15
	shrl	$16,%eax

	movzbl	%cl,%esi
	movzbl	%dl,%edi
	movzbl	%al,%ebp
	xorl	2(%r14,%rsi,8),%r10d
	xorl	2(%r14,%rdi,8),%r11d
	xorl	2(%r14,%rbp,8),%r12d

	movzbl	%dh,%esi
	movzbl	%ah,%edi
	movzbl	%bl,%ebp
	xorl	1(%r14,%rsi,8),%r10d
	xorl	1(%r14,%rdi,8),%r11d
	xorl	2(%r14,%rbp,8),%r8d

	movl	12(%r15),%edx
	movzbl	%bh,%edi
	movzbl	%ch,%ebp
	movl	0(%r15),%eax
	xorl	1(%r14,%rdi,8),%r12d
	xorl	1(%r14,%rbp,8),%r8d

	movl	4(%r15),%ebx
	movl	8(%r15),%ecx
	xorl	%r10d,%eax
	xorl	%r11d,%ebx
	xorl	%r12d,%ecx
	xorl	%r8d,%edx
	subl	$1,%r13d
	jnz	.Lenc_loop
	movzbl	%al,%esi
	movzbl	%bl,%edi
	movzbl	%cl,%ebp
	movzbl	2(%r14,%rsi,8),%r10d
	movzbl	2(%r14,%rdi,8),%r11d
	movzbl	2(%r14,%rbp,8),%r12d

	movzbl	%dl,%esi
	movzbl	%bh,%edi
	movzbl	%ch,%ebp
	movzbl	2(%r14,%rsi,8),%r8d
	movl	0(%r14,%rdi,8),%edi
	movl	0(%r14,%rbp,8),%ebp

	andl	$0xFF,%edi
	andl	$0xFF,%ebp

	xorl	%edi,%r10d
	xorl	%ebp,%r11d
	shrl	$16,%ecx

	movzbl	%dh,%esi
	movzbl	%ah,%edi
	shrl	$16,%edx
	movl	0(%r14,%rsi,8),%esi
	movl	0(%r14,%rdi,8),%edi

	andl	$0xFF,%esi
	andl	$0xFF,%edi
	shrl	$16,%ebx
	xorl	%esi,%r12d
	xorl	%edi,%r8d
	shrl	$16,%eax

	movzbl	%cl,%esi
	movzbl	%dl,%edi
	movzbl	%al,%ebp
	movl	0(%r14,%rsi,8),%esi
	movl	0(%r14,%rdi,8),%edi
	movl	0(%r14,%rbp,8),%ebp

	andl	$0xFF,%esi
	andl	$0xFF,%edi
	andl	$0xFF,%ebp

	xorl	%esi,%r10d
	xorl	%edi,%r11d
	xorl	%ebp,%r12d

	movzbl	%bl,%esi
	movzbl	%dh,%edi
	movzbl	%ah,%ebp
	movl	0(%r14,%rsi,8),%esi
	movl	2(%r14,%rdi,8),%edi
	movl	2(%r14,%rbp,8),%ebp

	andl	$0xFF,%esi
	andl	$0xFF,%edi
	andl	$0xFF,%ebp

	xorl	%esi,%r8d
	xorl	%edi,%r10d
	xorl	%ebp,%r11d

	movzbl	%bh,%esi
	movzbl	%ch,%edi
	movl	16+12(%r15),%edx
	movl	2(%r14,%rsi,8),%esi
	movl	2(%r14,%rdi,8),%edi
	movl	16+0(%r15),%eax

	andl	$0xFF,%esi
	andl	$0xFF,%edi

	xorl	%esi,%r12d
	xorl	%edi,%r8d

	movl	16+4(%r15),%ebx
	movl	16+8(%r15),%ecx
	xorl	%r10d,%eax
	xorl	%r11d,%ebx
	xorl	%r12d,%ecx
	xorl	%r8d,%edx
.byte	0xFF,0xFF
.size	_x86_64_AES_encrypt,.-_x86_64_AES_encrypt
.type	_x86_64_AES_encrypt_compact,@function
.align	16
_x86_64_AES_encrypt_compact:
.cfi_startproc	
	leaq	128(%r14),%r8
	movl	0-128(%r8),%edi
	movl	32-128(%r8),%ebp
	movl	64-128(%r8),%r10d
	movl	96-128(%r8),%r11d
	movl	128-128(%r8),%edi
	movl	160-128(%r8),%ebp
	movl	192-128(%r8),%r10d
	movl	224-128(%r8),%r11d
	jmp	.Lenc_loop_compact
.align	16
.Lenc_loop_compact:
	xorl	0(%r15),%eax
	xorl	4(%r15),%ebx
	xorl	8(%r15),%ecx
	xorl	12(%r15),%edx
	leaq	16(%r15),%r15
	movzbl	%al,%r10d
	movzbl	%bl,%r11d
	movzbl	%cl,%r12d
	movzbl	%dl,%r8d
	movzbl	%bh,%esi
	movzbl	%ch,%edi
	shrl	$16,%ecx
	movzbl	%dh,%ebp
	movzbl	(%r14,%r10,1),%r10d
	movzbl	(%r14,%r11,1),%r11d
	movzbl	(%r14,%r12,1),%r12d
	movzbl	(%r14,%r8,1),%r8d

	movzbl	(%r14,%rsi,1),%r9d
	movzbl	%ah,%esi
	movzbl	(%r14,%rdi,1),%r13d
	movzbl	%cl,%edi
	movzbl	(%r14,%rbp,1),%ebp
	movzbl	(%r14,%rsi,1),%esi

	shll	$8,%r9d
	shrl	$16,%edx
	shll	$8,%r13d
	xorl	%r9d,%r10d
	shrl	$16,%eax
	movzbl	%dl,%r9d
	shrl	$16,%ebx
	xorl	%r13d,%r11d
	shll	$8,%ebp
	movzbl	%al,%r13d
	movzbl	(%r14,%rdi,1),%edi
	xorl	%ebp,%r12d

	shll	$8,%esi
	movzbl	%bl,%ebp
	shll	$16,%edi
	xorl	%esi,%r8d
	movzbl	(%r14,%r9,1),%r9d
	movzbl	%dh,%esi
	movzbl	(%r14,%r13,1),%r13d
	xorl	%edi,%r10d

	shrl	$8,%ecx
	movzbl	%ah,%edi
	shll	$16,%r9d
	shrl	$8,%ebx
	shll	$16,%r13d
	xorl	%r9d,%r11d
	movzbl	(%r14,%rbp,1),%ebp
	movzbl	(%r14,%rsi,1),%esi
	movzbl	(%r14,%rdi,1),%edi
	movzbl	(%r14,%rcx,1),%edx
	movzbl	(%r14,%rbx,1),%ecx

	shll	$16,%ebp
	xorl	%r13d,%r12d
	shll	$24,%esi
	xorl	%ebp,%r8d
	shll	$24,%edi
	xorl	%esi,%r10d
	shll	$24,%edx
	xorl	%edi,%r11d
	shll	$24,%ecx
	movl	%r10d,%eax
	movl	%r11d,%ebx
	xorl	%r12d,%ecx
	xorl	%r8d,%edx
	cmpq	16(%rsp),%r15
	je	.Lenc_compact_done
	movl	$0xFF,%r10d
	movl	$0xFF,%r11d
	andl	%eax,%r10d
	andl	%ebx,%r11d
	movl	%r10d,%esi
	movl	%r11d,%edi
	shrl	$7,%r10d
	leal	(%rax,%rax,1),%r8d
	shrl	$7,%r11d
	leal	(%rbx,%rbx,1),%r9d
	subl	%r10d,%esi
	subl	%r11d,%edi
	andl	$0xFF,%r8d
	andl	$0xFF,%r9d
	andl	$0xFF,%esi
	andl	$0xFF,%edi
	movl	%eax,%r10d
	movl	%ebx,%r11d
	xorl	%esi,%r8d
	xorl	%edi,%r9d

	xorl	%r8d,%eax
	xorl	%r9d,%ebx
	movl	$0xFF,%r12d
	roll	$24,%eax
	movl	$0xFF,%ebp
	roll	$24,%ebx
	andl	%ecx,%r12d
	andl	%edx,%ebp
	xorl	%r8d,%eax
	xorl	%r9d,%ebx
	movl	%r12d,%esi
	rorl	$16,%r10d
	movl	%ebp,%edi
	rorl	$16,%r11d
	leal	(%rcx,%rcx,1),%r8d
	shrl	$7,%r12d
	xorl	%r10d,%eax
	shrl	$7,%ebp
	xorl	%r11d,%ebx
	rorl	$8,%r10d
	leal	(%rdx,%rdx,1),%r9d
	rorl	$8,%r11d
	subl	%r12d,%esi
	subl	%ebp,%edi
	xorl	%r10d,%eax
	xorl	%r11d,%ebx

	andl	$0xFF,%r8d
	andl	$0xFF,%r9d
	andl	$0xFF,%esi
	andl	$0xFF,%edi
	movl	%ecx,%r12d
	movl	%edx,%ebp
	xorl	%esi,%r8d
	xorl	%edi,%r9d

	rorl	$16,%r12d
	xorl	%r8d,%ecx
	rorl	$16,%ebp
	xorl	%r9d,%edx
	roll	$24,%ecx
	movl	0(%r14),%esi
	roll	$24,%edx
	xorl	%r8d,%ecx
	movl	64(%r14),%edi
	xorl	%r9d,%edx
	movl	128(%r14),%r8d
	xorl	%r12d,%ecx
	rorl	$8,%r12d
	xorl	%ebp,%edx
	rorl	$8,%ebp
	xorl	%r12d,%ecx
	movl	192(%r14),%r9d
	xorl	%ebp,%edx
	jmp	.Lenc_loop_compact
.align	16
.Lenc_compact_done:
	xorl	0(%r15),%eax
	xorl	4(%r15),%ebx
	xorl	8(%r15),%ecx
	xorl	12(%r15),%edx
.byte	0xFF,0xFF
.cfi_endproc	
.size	_x86_64_AES_encrypt_compact,.-_x86_64_AES_encrypt_compact
.globl	AES_encrypt
.type	AES_encrypt,@function
.align	16
.globl	asm_AES_encrypt
.hidden	asm_AES_encrypt
asm_AES_encrypt:
AES_encrypt:
.cfi_startproc	
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56


	leaq	-63(%rdx),%rcx
	andq	$-64,%rsp
	subq	%rsp,%rcx
	negq	%rcx
	andq	$0xFF,%rcx
	subq	%rcx,%rsp
	subq	$32,%rsp

	movq	%rsi,16(%rsp)
	movq	%rax,24(%rsp)
.cfi_escape	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.Lenc_prologue:

	movq	%rdx,%r15
	movl	240(%r15),%r13d

	movl	0(%rdi),%eax
	movl	4(%rdi),%ebx
	movl	8(%rdi),%ecx
	movl	12(%rdi),%edx

	shll	$4,%r13d
	leaq	(%r15,%r13,1),%rbp
	movq	%r15,(%rsp)
	movq	%rbp,8(%rsp)


	leaq	.LAES_Te+2048(%rip),%r14
	leaq	768(%rsp),%rbp
	subq	%r14,%rbp
	andq	$0xFF,%rbp
	leaq	(%r14,%rbp,1),%r14

	call	_x86_64_AES_encrypt_compact

	movq	16(%rsp),%r9
	movq	24(%rsp),%rsi
.cfi_def_cfa	%rsi,8
	movl	%eax,0(%r9)
	movl	%ebx,4(%r9)
	movl	%ecx,8(%r9)
	movl	%edx,12(%r9)

	movq	-48(%rsi),%r15
.cfi_restore	%r15
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lenc_epilogue:
	.byte	0xFF,0xFF
.cfi_endproc	
.size	AES_encrypt,.-AES_encrypt
.type	_x86_64_AES_decrypt,@function
.align	16
_x86_64_AES_decrypt:
	xorl	0(%r15),%eax
	xorl	4(%r15),%ebx
	xorl	8(%r15),%ecx
	xorl	12(%r15),%edx

	movl	240(%r15),%r13d
	subl	$1,%r13d
	jmp	.Ldec_loop
.align	16
.Ldec_loop:

	movzbl	%al,%esi
	movzbl	%bl,%edi
	movzbl	%cl,%ebp
	movl	0(%r14,%rsi,8),%r10d
	movl	0(%r14,%rdi,8),%r11d
	movl	0(%r14,%rbp,8),%r12d

	movzbl	%dh,%esi
	movzbl	%ah,%edi
	movzbl	%dl,%ebp
	xorl	3(%r14,%rsi,8),%r10d
	xorl	3(%r14,%rdi,8),%r11d
	movl	0(%r14,%rbp,8),%r8d

	movzbl	%bh,%esi
	shrl	$16,%eax
	movzbl	%ch,%ebp
	xorl	3(%r14,%rsi,8),%r12d
	shrl	$16,%edx
	xorl	3(%r14,%rbp,8),%r8d

	shrl	$16,%ebx
	leaq	16(%r15),%r15
	shrl	$16,%ecx

	movzbl	%cl,%esi
	movzbl	%dl,%edi
	movzbl	%al,%ebp
	xorl	2(%r14,%rsi,8),%r10d
	xorl	2(%r14,%rdi,8),%r11d
	xorl	2(%r14,%rbp,8),%r12d

	movzbl	%bh,%esi
	movzbl	%ch,%edi
	movzbl	%bl,%ebp
	xorl	1(%r14,%rsi,8),%r10d
	xorl	1(%r14,%rdi,8),%r11d
	xorl	2(%r14,%rbp,8),%r8d

	movzbl	%dh,%esi
	movl	12(%r15),%edx
	movzbl	%ah,%ebp
	xorl	1(%r14,%rsi,8),%r12d
	movl	0(%r15),%eax
	xorl	1(%r14,%rbp,8),%r8d

	xorl	%r10d,%eax
	movl	4(%r15),%ebx
	movl	8(%r15),%ecx
	xorl	%r12d,%ecx
	xorl	%r11d,%ebx
	xorl	%r8d,%edx
	subl	$1,%r13d
	jnz	.Ldec_loop
	leaq	2048(%r14),%r14
	movzbl	%al,%esi
	movzbl	%bl,%edi
	movzbl	%cl,%ebp
	movzbl	(%r14,%rsi,1),%r10d
	movzbl	(%r14,%rdi,1),%r11d
	movzbl	(%r14,%rbp,1),%r12d

	movzbl	%dl,%esi
	movzbl	%dh,%edi
	movzbl	%ah,%ebp
	movzbl	(%r14,%rsi,1),%r8d
	movzbl	(%r14,%rdi,1),%edi
	movzbl	(%r14,%rbp,1),%ebp

	shll	$8,%edi
	shll	$8,%ebp

	xorl	%edi,%r10d
	xorl	%ebp,%r11d
	shrl	$16,%edx

	movzbl	%bh,%esi
	movzbl	%ch,%edi
	shrl	$16,%eax
	movzbl	(%r14,%rsi,1),%esi
	movzbl	(%r14,%rdi,1),%edi

	shll	$8,%esi
	shll	$8,%edi
	shrl	$16,%ebx
	xorl	%esi,%r12d
	xorl	%edi,%r8d
	shrl	$16,%ecx

	movzbl	%cl,%esi
	movzbl	%dl,%edi
	movzbl	%al,%ebp
	movzbl	(%r14,%rsi,1),%esi
	movzbl	(%r14,%rdi,1),%edi
	movzbl	(%r14,%rbp,1),%ebp

	shll	$16,%esi
	shll	$16,%edi
	shll	$16,%ebp

	xorl	%esi,%r10d
	xorl	%edi,%r11d
	xorl	%ebp,%r12d

	movzbl	%bl,%esi
	movzbl	%bh,%edi
	movzbl	%ch,%ebp
	movzbl	(%r14,%rsi,1),%esi
	movzbl	(%r14,%rdi,1),%edi
	movzbl	(%r14,%rbp,1),%ebp

	shll	$16,%esi
	shll	$24,%edi
	shll	$24,%ebp

	xorl	%esi,%r8d
	xorl	%edi,%r10d
	xorl	%ebp,%r11d

	movzbl	%dh,%esi
	movzbl	%ah,%edi
	movl	16+12(%r15),%edx
	movzbl	(%r14,%rsi,1),%esi
	movzbl	(%r14,%rdi,1),%edi
	movl	16+0(%r15),%eax

	shll	$24,%esi
	shll	$24,%edi

	xorl	%esi,%r12d
	xorl	%edi,%r8d

	movl	16+4(%r15),%ebx
	movl	16+8(%r15),%ecx
	leaq	-2048(%r14),%r14
	xorl	%r10d,%eax
	xorl	%r11d,%ebx
	xorl	%r12d,%ecx
	xorl	%r8d,%edx
.byte	0xFF,0xFF
.size	_x86_64_AES_decrypt,.-_x86_64_AES_decrypt
.type	_x86_64_AES_decrypt_compact,@function
.align	16
_x86_64_AES_decrypt_compact:
.cfi_startproc	
	leaq	128(%r14),%r8
	movl	0-128(%r8),%edi
	movl	32-128(%r8),%ebp
	movl	64-128(%r8),%r10d
	movl	96-128(%r8),%r11d
	movl	128-128(%r8),%edi
	movl	160-128(%r8),%ebp
	movl	192-128(%r8),%r10d
	movl	224-128(%r8),%r11d
	jmp	.Ldec_loop_compact

.align	16
.Ldec_loop_compact:
	xorl	0(%r15),%eax
	xorl	4(%r15),%ebx
	xorl	8(%r15),%ecx
	xorl	12(%r15),%edx
	leaq	16(%r15),%r15
	movzbl	%al,%r10d
	movzbl	%bl,%r11d
	movzbl	%cl,%r12d
	movzbl	%dl,%r8d
	movzbl	%dh,%esi
	movzbl	%ah,%edi
	shrl	$16,%edx
	movzbl	%bh,%ebp
	movzbl	(%r14,%r10,1),%r10d
	movzbl	(%r14,%r11,1),%r11d
	movzbl	(%r14,%r12,1),%r12d
	movzbl	(%r14,%r8,1),%r8d

	movzbl	(%r14,%rsi,1),%r9d
	movzbl	%ch,%esi
	movzbl	(%r14,%rdi,1),%r13d
	movzbl	(%r14,%rbp,1),%ebp
	movzbl	(%r14,%rsi,1),%esi

	shrl	$16,%ecx
	shll	$8,%r13d
	shll	$8,%r9d
	movzbl	%cl,%edi
	shrl	$16,%eax
	xorl	%r9d,%r10d
	shrl	$16,%ebx
	movzbl	%dl,%r9d

	shll	$8,%ebp
	xorl	%r13d,%r11d
	shll	$8,%esi
	movzbl	%al,%r13d
	movzbl	(%r14,%rdi,1),%edi
	xorl	%ebp,%r12d
	movzbl	%bl,%ebp

	shll	$16,%edi
	xorl	%esi,%r8d
	movzbl	(%r14,%r9,1),%r9d
	movzbl	%bh,%esi
	movzbl	(%r14,%rbp,1),%ebp
	xorl	%edi,%r10d
	movzbl	(%r14,%r13,1),%r13d
	movzbl	%ch,%edi

	shll	$16,%ebp
	shll	$16,%r9d
	shll	$16,%r13d
	xorl	%ebp,%r8d
	movzbl	%dh,%ebp
	xorl	%r9d,%r11d
	shrl	$8,%eax
	xorl	%r13d,%r12d

	movzbl	(%r14,%rsi,1),%esi
	movzbl	(%r14,%rdi,1),%ebx
	movzbl	(%r14,%rbp,1),%ecx
	movzbl	(%r14,%rax,1),%edx

	movl	%r10d,%eax
	shll	$24,%esi
	shll	$24,%ebx
	shll	$24,%ecx
	xorl	%esi,%eax
	shll	$24,%edx
	xorl	%r11d,%ebx
	xorl	%r12d,%ecx
	xorl	%r8d,%edx
	cmpq	16(%rsp),%r15
	je	.Ldec_compact_done

	movq	256+0(%r14),%rsi
	shlq	$32,%rbx
	shlq	$32,%rdx
	movq	256+8(%r14),%rdi
	orq	%rbx,%rax
	orq	%rdx,%rcx
	movq	256+16(%r14),%rbp
	movq	%rsi,%r9
	movq	%rsi,%r12
	andq	%rax,%r9
	andq	%rcx,%r12
	movq	%r9,%rbx
	movq	%r12,%rdx
	shrq	$7,%r9
	leaq	(%rax,%rax,1),%r8
	shrq	$7,%r12
	leaq	(%rcx,%rcx,1),%r11
	subq	%r9,%rbx
	subq	%r12,%rdx
	andq	%rdi,%r8
	andq	%rdi,%r11
	andq	%rbp,%rbx
	andq	%rbp,%rdx
	xorq	%rbx,%r8
	xorq	%rdx,%r11
	movq	%rsi,%r10
	movq	%rsi,%r13

	andq	%r8,%r10
	andq	%r11,%r13
	movq	%r10,%rbx
	movq	%r13,%rdx
	shrq	$7,%r10
	leaq	(%r8,%r8,1),%r9
	shrq	$7,%r13
	leaq	(%r11,%r11,1),%r12
	subq	%r10,%rbx
	subq	%r13,%rdx
	andq	%rdi,%r9
	andq	%rdi,%r12
	andq	%rbp,%rbx
	andq	%rbp,%rdx
	xorq	%rbx,%r9
	xorq	%rdx,%r12
	movq	%rsi,%r10
	movq	%rsi,%r13

	andq	%r9,%r10
	andq	%r12,%r13
	movq	%r10,%rbx
	movq	%r13,%rdx
	shrq	$7,%r10
	xorq	%rax,%r8
	shrq	$7,%r13
	xorq	%rcx,%r11
	subq	%r10,%rbx
	subq	%r13,%rdx
	leaq	(%r9,%r9,1),%r10
	leaq	(%r12,%r12,1),%r13
	xorq	%rax,%r9
	xorq	%rcx,%r12
	andq	%rdi,%r10
	andq	%rdi,%r13
	andq	%rbp,%rbx
	andq	%rbp,%rdx
	xorq	%rbx,%r10
	xorq	%rdx,%r13

	xorq	%r10,%rax
	xorq	%r13,%rcx
	xorq	%r10,%r8
	xorq	%r13,%r11
	movq	%rax,%rbx
	movq	%rcx,%rdx
	xorq	%r10,%r9
	shrq	$32,%rbx
	xorq	%r13,%r12
	shrq	$32,%rdx
	xorq	%r8,%r10
	roll	$8,%eax
	xorq	%r11,%r13
	roll	$8,%ecx
	xorq	%r9,%r10
	roll	$8,%ebx
	xorq	%r12,%r13

	roll	$8,%edx
	xorl	%r10d,%eax
	shrq	$32,%r10
	xorl	%r13d,%ecx
	shrq	$32,%r13
	xorl	%r10d,%ebx
	xorl	%r13d,%edx

	movq	%r8,%r10
	roll	$24,%r8d
	movq	%r11,%r13
	roll	$24,%r11d
	shrq	$32,%r10
	xorl	%r8d,%eax
	shrq	$32,%r13
	xorl	%r11d,%ecx
	roll	$24,%r10d
	movq	%r9,%r8
	roll	$24,%r13d
	movq	%r12,%r11
	shrq	$32,%r8
	xorl	%r10d,%ebx
	shrq	$32,%r11
	xorl	%r13d,%edx

	movq	0(%r14),%rsi
	roll	$16,%r9d
	movq	64(%r14),%rdi
	roll	$16,%r12d
	movq	128(%r14),%rbp
	roll	$16,%r8d
	movq	192(%r14),%r10
	xorl	%r9d,%eax
	roll	$16,%r11d
	xorl	%r12d,%ecx
	movq	256(%r14),%r13
	xorl	%r8d,%ebx
	xorl	%r11d,%edx
	jmp	.Ldec_loop_compact
.align	16
.Ldec_compact_done:
	xorl	0(%r15),%eax
	xorl	4(%r15),%ebx
	xorl	8(%r15),%ecx
	xorl	12(%r15),%edx
.byte	0xFF,0xFF
.cfi_endproc	
.size	_x86_64_AES_decrypt_compact,.-_x86_64_AES_decrypt_compact
.globl	AES_decrypt
.type	AES_decrypt,@function
.align	16
.globl	asm_AES_decrypt
.hidden	asm_AES_decrypt
asm_AES_decrypt:
AES_decrypt:
.cfi_startproc	
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56


	leaq	-63(%rdx),%rcx
	andq	$-64,%rsp
	subq	%rsp,%rcx
	negq	%rcx
	andq	$0xFF,%rcx
	subq	%rcx,%rsp
	subq	$32,%rsp

	movq	%rsi,16(%rsp)
	movq	%rax,24(%rsp)
.cfi_escape	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.Ldec_prologue:

	movq	%rdx,%r15
	movl	240(%r15),%r13d

	movl	0(%rdi),%eax
	movl	4(%rdi),%ebx
	movl	8(%rdi),%ecx
	movl	12(%rdi),%edx

	shll	$4,%r13d
	leaq	(%r15,%r13,1),%rbp
	movq	%r15,(%rsp)
	movq	%rbp,8(%rsp)


	leaq	.LAES_Td+2048(%rip),%r14
	leaq	768(%rsp),%rbp
	subq	%r14,%rbp
	andq	$0xFF,%rbp
	leaq	(%r14,%rbp,1),%r14
	shrq	$3,%rbp
	addq	%rbp,%r14

	call	_x86_64_AES_decrypt_compact

	movq	16(%rsp),%r9
	movq	24(%rsp),%rsi
.cfi_def_cfa	%rsi,8
	movl	%eax,0(%r9)
	movl	%ebx,4(%r9)
	movl	%ecx,8(%r9)
	movl	%edx,12(%r9)

	movq	-48(%rsi),%r15
.cfi_restore	%r15
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Ldec_epilogue:
	.byte	0xFF,0xFF
.cfi_endproc	
.size	AES_decrypt,.-AES_decrypt
.globl	AES_set_encrypt_key
.type	AES_set_encrypt_key,@function
.align	16
AES_set_encrypt_key:
.cfi_startproc	
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
	subq	$8,%rsp
.cfi_adjust_cfa_offset	8
.Lenc_key_prologue:

	call	_x86_64_AES_set_encrypt_key

	movq	40(%rsp),%rbp
.cfi_restore	%rbp
	movq	48(%rsp),%rbx
.cfi_restore	%rbx
	addq	$56,%rsp
.cfi_adjust_cfa_offset	-56
.Lenc_key_epilogue:
	.byte	0xFF,0xFF
.cfi_endproc	
.size	AES_set_encrypt_key,.-AES_set_encrypt_key

.type	_x86_64_AES_set_encrypt_key,@function
.align	16
_x86_64_AES_set_encrypt_key:
.cfi_startproc	
	movl	%esi,%ecx
	movq	%rdi,%rsi
	movq	%rdx,%rdi

	testq	$-1,%rsi
	jz	.Lbadpointer
	testq	$-1,%rdi
	jz	.Lbadpointer

	leaq	.LAES_Te(%rip),%rbp
	leaq	2048+128(%rbp),%rbp


	movl	0-128(%rbp),%eax
	movl	32-128(%rbp),%ebx
	movl	64-128(%rbp),%r8d
	movl	96-128(%rbp),%edx
	movl	128-128(%rbp),%eax
	movl	160-128(%rbp),%ebx
	movl	192-128(%rbp),%r8d
	movl	224-128(%rbp),%edx

	cmpl	$128,%ecx
	je	.L10rounds
	cmpl	$192,%ecx
	je	.L12rounds
	cmpl	$256,%ecx
	je	.L14rounds
	movq	$-2,%rax
	jmp	.Lexit

.L10rounds:
	movq	0(%rsi),%rax
	movq	8(%rsi),%rdx
	movq	%rax,0(%rdi)
	movq	%rdx,8(%rdi)

	shrq	$32,%rdx
	xorl	%ecx,%ecx
	jmp	.L10shortcut
.align	4
.L10loop:
	movl	0(%rdi),%eax
	movl	12(%rdi),%edx
.L10shortcut:
	movzbl	%dl,%esi
	movzbl	-128(%rbp,%rsi,1),%ebx
	movzbl	%dh,%esi
	shll	$24,%ebx
	xorl	%ebx,%eax

	movzbl	-128(%rbp,%rsi,1),%ebx
	shrl	$16,%edx
	movzbl	%dl,%esi
	xorl	%ebx,%eax

	movzbl	-128(%rbp,%rsi,1),%ebx
	movzbl	%dh,%esi
	shll	$8,%ebx
	xorl	%ebx,%eax

	movzbl	-128(%rbp,%rsi,1),%ebx
	shll	$16,%ebx
	xorl	%ebx,%eax

	xorl	1024-128(%rbp,%rcx,4),%eax
	movl	%eax,16(%rdi)
	xorl	4(%rdi),%eax
	movl	%eax,20(%rdi)
	xorl	8(%rdi),%eax
	movl	%eax,24(%rdi)
	xorl	12(%rdi),%eax
	movl	%eax,28(%rdi)
	addl	$1,%ecx
	leaq	16(%rdi),%rdi
	cmpl	$10,%ecx
	jl	.L10loop

	movl	$10,80(%rdi)
	xorq	%rax,%rax
	jmp	.Lexit

.L12rounds:
	movq	0(%rsi),%rax
	movq	8(%rsi),%rbx
	movq	16(%rsi),%rdx
	movq	%rax,0(%rdi)
	movq	%rbx,8(%rdi)
	movq	%rdx,16(%rdi)

	shrq	$32,%rdx
	xorl	%ecx,%ecx
	jmp	.L12shortcut
.align	4
.L12loop:
	movl	0(%rdi),%eax
	movl	20(%rdi),%edx
.L12shortcut:
	movzbl	%dl,%esi
	movzbl	-128(%rbp,%rsi,1),%ebx
	movzbl	%dh,%esi
	shll	$24,%ebx
	xorl	%ebx,%eax

	movzbl	-128(%rbp,%rsi,1),%ebx
	shrl	$16,%edx
	movzbl	%dl,%esi
	xorl	%ebx,%eax

	movzbl	-128(%rbp,%rsi,1),%ebx
	movzbl	%dh,%esi
	shll	$8,%ebx
	xorl	%ebx,%eax

	movzbl	-128(%rbp,%rsi,1),%ebx
	shll	$16,%ebx
	xorl	%ebx,%eax

	xorl	1024-128(%rbp,%rcx,4),%eax
	movl	%eax,24(%rdi)
	xorl	4(%rdi),%eax
	movl	%eax,28(%rdi)
	xorl	8(%rdi),%eax
	movl	%eax,32(%rdi)
	xorl	12(%rdi),%eax
	movl	%eax,36(%rdi)

	cmpl	$7,%ecx
	je	.L12break
	addl	$1,%ecx

	xorl	16(%rdi),%eax
	movl	%eax,40(%rdi)
	xorl	20(%rdi),%eax
	movl	%eax,44(%rdi)

	leaq	24(%rdi),%rdi
	jmp	.L12loop
.L12break:
	movl	$12,72(%rdi)
	xorq	%rax,%rax
	jmp	.Lexit

.L14rounds:
	movq	0(%rsi),%rax
	movq	8(%rsi),%rbx
	movq	16(%rsi),%rcx
	movq	24(%rsi),%rdx
	movq	%rax,0(%rdi)
	movq	%rbx,8(%rdi)
	movq	%rcx,16(%rdi)
	movq	%rdx,24(%rdi)

	shrq	$32,%rdx
	xorl	%ecx,%ecx
	jmp	.L14shortcut
.align	4
.L14loop:
	movl	0(%rdi),%eax
	movl	28(%rdi),%edx
.L14shortcut:
	movzbl	%dl,%esi
	movzbl	-128(%rbp,%rsi,1),%ebx
	movzbl	%dh,%esi
	shll	$24,%ebx
	xorl	%ebx,%eax

	movzbl	-128(%rbp,%rsi,1),%ebx
	shrl	$16,%edx
	movzbl	%dl,%esi
	xorl	%ebx,%eax

	movzbl	-128(%rbp,%rsi,1),%ebx
	movzbl	%dh,%esi
	shll	$8,%ebx
	xorl	%ebx,%eax

	movzbl	-128(%rbp,%rsi,1),%ebx
	shll	$16,%ebx
	xorl	%ebx,%eax

	xorl	1024-128(%rbp,%rcx,4),%eax
	movl	%eax,32(%rdi)
	xorl	4(%rdi),%eax
	movl	%eax,36(%rdi)
	xorl	8(%rdi),%eax
	movl	%eax,40(%rdi)
	xorl	12(%rdi),%eax
	movl	%eax,44(%rdi)

	cmpl	$6,%ecx
	je	.L14break
	addl	$1,%ecx

	movl	%eax,%edx
	movl	16(%rdi),%eax
	movzbl	%dl,%esi
	movzbl	-128(%rbp,%rsi,1),%ebx
	movzbl	%dh,%esi
	xorl	%ebx,%eax

	movzbl	-128(%rbp,%rsi,1),%ebx
	shrl	$16,%edx
	shll	$8,%ebx
	movzbl	%dl,%esi
	xorl	%ebx,%eax

	movzbl	-128(%rbp,%rsi,1),%ebx
	movzbl	%dh,%esi
	shll	$16,%ebx
	xorl	%ebx,%eax

	movzbl	-128(%rbp,%rsi,1),%ebx
	shll	$24,%ebx
	xorl	%ebx,%eax

	movl	%eax,48(%rdi)
	xorl	20(%rdi),%eax
	movl	%eax,52(%rdi)
	xorl	24(%rdi),%eax
	movl	%eax,56(%rdi)
	xorl	28(%rdi),%eax
	movl	%eax,60(%rdi)

	leaq	32(%rdi),%rdi
	jmp	.L14loop
.L14break:
	movl	$14,48(%rdi)
	xorq	%rax,%rax
	jmp	.Lexit

.Lbadpointer:
	movq	$-1,%rax
.Lexit:
.byte	0xFF,0xFF
.cfi_endproc	
.size	_x86_64_AES_set_encrypt_key,.-_x86_64_AES_set_encrypt_key
.globl	AES_set_decrypt_key
.type	AES_set_decrypt_key,@function
.align	16
AES_set_decrypt_key:
.cfi_startproc	
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
	pushq	%rdx
.cfi_adjust_cfa_offset	8
.Ldec_key_prologue:

	call	_x86_64_AES_set_encrypt_key
	movq	(%rsp),%r8
	cmpl	$0,%eax
	jne	.Labort

	movl	240(%r8),%r14d
	xorq	%rdi,%rdi
	leaq	(%rdi,%r14,4),%rcx
	movq	%r8,%rsi
	leaq	(%r8,%rcx,4),%rdi
.align	4
.Linvert:
	movq	0(%rsi),%rax
	movq	8(%rsi),%rbx
	movq	0(%rdi),%rcx
	movq	8(%rdi),%rdx
	movq	%rax,0(%rdi)
	movq	%rbx,8(%rdi)
	movq	%rcx,0(%rsi)
	movq	%rdx,8(%rsi)
	leaq	16(%rsi),%rsi
	leaq	-16(%rdi),%rdi
	cmpq	%rsi,%rdi
	jne	.Linvert

	leaq	.LAES_Te+2048+1024(%rip),%rax

	movq	40(%rax),%rsi
	movq	48(%rax),%rdi
	movq	56(%rax),%rbp

	movq	%r8,%r15
	subl	$1,%r14d
.align	4
.Lpermute:
	leaq	16(%r15),%r15
	movq	0(%r15),%rax
	movq	8(%r15),%rcx
	movq	%rsi,%r9
	movq	%rsi,%r12
	andq	%rax,%r9
	andq	%rcx,%r12
	movq	%r9,%rbx
	movq	%r12,%rdx
	shrq	$7,%r9
	leaq	(%rax,%rax,1),%r8
	shrq	$7,%r12
	leaq	(%rcx,%rcx,1),%r11
	subq	%r9,%rbx
	subq	%r12,%rdx
	andq	%rdi,%r8
	andq	%rdi,%r11
	andq	%rbp,%rbx
	andq	%rbp,%rdx
	xorq	%rbx,%r8
	xorq	%rdx,%r11
	movq	%rsi,%r10
	movq	%rsi,%r13

	andq	%r8,%r10
	andq	%r11,%r13
	movq	%r10,%rbx
	movq	%r13,%rdx
	shrq	$7,%r10
	leaq	(%r8,%r8,1),%r9
	shrq	$7,%r13
	leaq	(%r11,%r11,1),%r12
	subq	%r10,%rbx
	subq	%r13,%rdx
	andq	%rdi,%r9
	andq	%rdi,%r12
	andq	%rbp,%rbx
	andq	%rbp,%rdx
	xorq	%rbx,%r9
	xorq	%rdx,%r12
	movq	%rsi,%r10
	movq	%rsi,%r13

	andq	%r9,%r10
	andq	%r12,%r13
	movq	%r10,%rbx
	movq	%r13,%rdx
	shrq	$7,%r10
	xorq	%rax,%r8
	shrq	$7,%r13
	xorq	%rcx,%r11
	subq	%r10,%rbx
	subq	%r13,%rdx
	leaq	(%r9,%r9,1),%r10
	leaq	(%r12,%r12,1),%r13
	xorq	%rax,%r9
	xorq	%rcx,%r12
	andq	%rdi,%r10
	andq	%rdi,%r13
	andq	%rbp,%rbx
	andq	%rbp,%rdx
	xorq	%rbx,%r10
	xorq	%rdx,%r13

	xorq	%r10,%rax
	xorq	%r13,%rcx
	xorq	%r10,%r8
	xorq	%r13,%r11
	movq	%rax,%rbx
	movq	%rcx,%rdx
	xorq	%r10,%r9
	shrq	$32,%rbx
	xorq	%r13,%r12
	shrq	$32,%rdx
	xorq	%r8,%r10
	roll	$8,%eax
	xorq	%r11,%r13
	roll	$8,%ecx
	xorq	%r9,%r10
	roll	$8,%ebx
	xorq	%r12,%r13

	roll	$8,%edx
	xorl	%r10d,%eax
	shrq	$32,%r10
	xorl	%r13d,%ecx
	shrq	$32,%r13
	xorl	%r10d,%ebx
	xorl	%r13d,%edx

	movq	%r8,%r10
	roll	$24,%r8d
	movq	%r11,%r13
	roll	$24,%r11d
	shrq	$32,%r10
	xorl	%r8d,%eax
	shrq	$32,%r13
	xorl	%r11d,%ecx
	roll	$24,%r10d
	movq	%r9,%r8
	roll	$24,%r13d
	movq	%r12,%r11
	shrq	$32,%r8
	xorl	%r10d,%ebx
	shrq	$32,%r11
	xorl	%r13d,%edx


	roll	$16,%r9d

	roll	$16,%r12d

	roll	$16,%r8d

	xorl	%r9d,%eax
	roll	$16,%r11d
	xorl	%r12d,%ecx

	xorl	%r8d,%ebx
	xorl	%r11d,%edx
	movl	%eax,0(%r15)
	movl	%ebx,4(%r15)
	movl	%ecx,8(%r15)
	movl	%edx,12(%r15)
	subl	$1,%r14d
	jnz	.Lpermute

	xorq	%rax,%rax
.Labort:
	movq	8(%rsp),%r15
.cfi_restore	%r15
	movq	16(%rsp),%r14
.cfi_restore	%r14
	movq	24(%rsp),%r13
.cfi_restore	%r13
	movq	32(%rsp),%r12
.cfi_restore	%r12
	movq	40(%rsp),%rbp
.cfi_restore	%rbp
	movq	48(%rsp),%rbx
.cfi_restore	%rbx
	addq	$56,%rsp
.cfi_adjust_cfa_offset	-56
.Ldec_key_epilogue:
	.byte	0xFF,0xFF
.cfi_endproc	
.size	AES_set_decrypt_key,.-AES_set_decrypt_key
.globl	AES_cbc_encrypt
.type	AES_cbc_encrypt,@function
.align	16

.globl	asm_AES_cbc_encrypt
.hidden	asm_AES_cbc_encrypt
asm_AES_cbc_encrypt:
AES_cbc_encrypt:
.cfi_startproc	
	cmpq	$0,%rdx
	je	.Lcbc_epilogue
	pushfq


.cfi_adjust_cfa_offset	8
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-24
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-32
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-40
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-48
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-56
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-64
.Lcbc_prologue:

	cld
	movl	%r9d,%r9d

	leaq	.LAES_Te(%rip),%r14
	leaq	.LAES_Td(%rip),%r10
	cmpq	$0,%r9
	cmoveq	%r10,%r14

.cfi_remember_state	
	movl	OPENSSL_ia32cap_P(%rip),%r10d
	cmpq	$512,%rdx
	jb	.Lcbc_slow_prologue
	testq	$15,%rdx
	jnz	.Lcbc_slow_prologue
	btl	$28,%r10d
	jc	.Lcbc_slow_prologue


	leaq	-88-248(%rsp),%r15
	andq	$-64,%r15


	movq	%r14,%r10
	leaq	2304(%r14),%r11
	movq	%r15,%r12
	andq	$0xFF,%r10
	andq	$0xFF,%r11
	andq	$0xFF,%r12

	cmpq	%r11,%r12
	jb	.Lcbc_te_break_out
	subq	%r11,%r12
	subq	%r12,%r15
	jmp	.Lcbc_te_ok
.Lcbc_te_break_out:
	subq	%r10,%r12
	andq	$0xFF,%r12
	addq	$320,%r12
	subq	%r12,%r15
.align	4
.Lcbc_te_ok:

	xchgq	%rsp,%r15
.cfi_def_cfa_register	%r15

	movq	%r15,16(%rsp)
.cfi_escape	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.Lcbc_fast_body:
	movq	%rdi,24(%rsp)
	movq	%rsi,32(%rsp)
	movq	%rdx,40(%rsp)
	movq	%rcx,48(%rsp)
	movq	%r8,56(%rsp)
	movl	$0,80+240(%rsp)
	movq	%r8,%rbp
	movq	%r9,%rbx
	movq	%rsi,%r9
	movq	%rdi,%r8
	movq	%rcx,%r15

	movl	240(%r15),%eax

	movq	%r15,%r10
	subq	%r14,%r10
	andq	$0xFF,%r10
	cmpq	$2304,%r10
	jb	.Lcbc_do_ecopy
	cmpq	$4096-248,%r10
	jb	.Lcbc_skip_ecopy
.align	4
.Lcbc_do_ecopy:
	movq	%r15,%rsi
	leaq	80(%rsp),%rdi
	leaq	80(%rsp),%r15
	movl	$30,%ecx
.long	0xFF
	movl	%eax,(%rdi)
.Lcbc_skip_ecopy:
	movq	%r15,0(%rsp)

	movl	$18,%ecx
.align	4
.Lcbc_prefetch_te:
	movq	0(%r14),%r10
	movq	32(%r14),%r11
	movq	64(%r14),%r12
	movq	96(%r14),%r13
	leaq	128(%r14),%r14
	subl	$1,%ecx
	jnz	.Lcbc_prefetch_te
	leaq	-2304(%r14),%r14

	cmpq	$0,%rbx
	je	.LFAST_DECRYPT


	movl	0(%rbp),%eax
	movl	4(%rbp),%ebx
	movl	8(%rbp),%ecx
	movl	12(%rbp),%edx

.align	4
.Lcbc_fast_enc_loop:
	xorl	0(%r8),%eax
	xorl	4(%r8),%ebx
	xorl	8(%r8),%ecx
	xorl	12(%r8),%edx
	movq	0(%rsp),%r15
	movq	%r8,24(%rsp)

	call	_x86_64_AES_encrypt

	movq	24(%rsp),%r8
	movq	40(%rsp),%r10
	movl	%eax,0(%r9)
	movl	%ebx,4(%r9)
	movl	%ecx,8(%r9)
	movl	%edx,12(%r9)

	leaq	16(%r8),%r8
	leaq	16(%r9),%r9
	subq	$16,%r10
	testq	$-16,%r10
	movq	%r10,40(%rsp)
	jnz	.Lcbc_fast_enc_loop
	movq	56(%rsp),%rbp
	movl	%eax,0(%rbp)
	movl	%ebx,4(%rbp)
	movl	%ecx,8(%rbp)
	movl	%edx,12(%rbp)

	jmp	.Lcbc_fast_cleanup


.align	16
.LFAST_DECRYPT:
	cmpq	%r8,%r9
	je	.Lcbc_fast_dec_in_place

	movq	%rbp,64(%rsp)
.align	4
.Lcbc_fast_dec_loop:
	movl	0(%r8),%eax
	movl	4(%r8),%ebx
	movl	8(%r8),%ecx
	movl	12(%r8),%edx
	movq	0(%rsp),%r15
	movq	%r8,24(%rsp)

	call	_x86_64_AES_decrypt

	movq	64(%rsp),%rbp
	movq	24(%rsp),%r8
	movq	40(%rsp),%r10
	xorl	0(%rbp),%eax
	xorl	4(%rbp),%ebx
	xorl	8(%rbp),%ecx
	xorl	12(%rbp),%edx
	movq	%r8,%rbp

	subq	$16,%r10
	movq	%r10,40(%rsp)
	movq	%rbp,64(%rsp)

	movl	%eax,0(%r9)
	movl	%ebx,4(%r9)
	movl	%ecx,8(%r9)
	movl	%edx,12(%r9)

	leaq	16(%r8),%r8
	leaq	16(%r9),%r9
	jnz	.Lcbc_fast_dec_loop
	movq	56(%rsp),%r12
	movq	0(%rbp),%r10
	movq	8(%rbp),%r11
	movq	%r10,0(%r12)
	movq	%r11,8(%r12)
	jmp	.Lcbc_fast_cleanup

.align	16
.Lcbc_fast_dec_in_place:
	movq	0(%rbp),%r10
	movq	8(%rbp),%r11
	movq	%r10,0+64(%rsp)
	movq	%r11,8+64(%rsp)
.align	4
.Lcbc_fast_dec_in_place_loop:
	movl	0(%r8),%eax
	movl	4(%r8),%ebx
	movl	8(%r8),%ecx
	movl	12(%r8),%edx
	movq	0(%rsp),%r15
	movq	%r8,24(%rsp)

	call	_x86_64_AES_decrypt

	movq	24(%rsp),%r8
	movq	40(%rsp),%r10
	xorl	0+64(%rsp),%eax
	xorl	4+64(%rsp),%ebx
	xorl	8+64(%rsp),%ecx
	xorl	12+64(%rsp),%edx

	movq	0(%r8),%r11
	movq	8(%r8),%r12
	subq	$16,%r10
	jz	.Lcbc_fast_dec_in_place_done

	movq	%r11,0+64(%rsp)
	movq	%r12,8+64(%rsp)

	movl	%eax,0(%r9)
	movl	%ebx,4(%r9)
	movl	%ecx,8(%r9)
	movl	%edx,12(%r9)

	leaq	16(%r8),%r8
	leaq	16(%r9),%r9
	movq	%r10,40(%rsp)
	jmp	.Lcbc_fast_dec_in_place_loop
.Lcbc_fast_dec_in_place_done:
	movq	56(%rsp),%rdi
	movq	%r11,0(%rdi)
	movq	%r12,8(%rdi)

	movl	%eax,0(%r9)
	movl	%ebx,4(%r9)
	movl	%ecx,8(%r9)
	movl	%edx,12(%r9)

.align	4
.Lcbc_fast_cleanup:
	cmpl	$0,80+240(%rsp)
	leaq	80(%rsp),%rdi
	je	.Lcbc_exit
	movl	$30,%ecx
	xorq	%rax,%rax
.long	0xFF

	jmp	.Lcbc_exit


.align	16
.Lcbc_slow_prologue:
.cfi_restore_state	

	leaq	-88(%rsp),%rbp
	andq	$-64,%rbp

	leaq	-88-63(%rcx),%r10
	subq	%rbp,%r10
	negq	%r10
	andq	$0xFF,%r10
	subq	%r10,%rbp

	xchgq	%rsp,%rbp
.cfi_def_cfa_register	%rbp

	movq	%rbp,16(%rsp)
.cfi_escape	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.Lcbc_slow_body:




	movq	%r8,56(%rsp)
	movq	%r8,%rbp
	movq	%r9,%rbx
	movq	%rsi,%r9
	movq	%rdi,%r8
	movq	%rcx,%r15
	movq	%rdx,%r10

	movl	240(%r15),%eax
	movq	%r15,0(%rsp)
	shll	$4,%eax
	leaq	(%r15,%rax,1),%rax
	movq	%rax,8(%rsp)


	leaq	2048(%r14),%r14
	leaq	768-8(%rsp),%rax
	subq	%r14,%rax
	andq	$0xFF,%rax
	leaq	(%r14,%rax,1),%r14

	cmpq	$0,%rbx
	je	.LSLOW_DECRYPT


	testq	$-16,%r10
	movl	0(%rbp),%eax
	movl	4(%rbp),%ebx
	movl	8(%rbp),%ecx
	movl	12(%rbp),%edx
	jz	.Lcbc_slow_enc_tail

.align	4
.Lcbc_slow_enc_loop:
	xorl	0(%r8),%eax
	xorl	4(%r8),%ebx
	xorl	8(%r8),%ecx
	xorl	12(%r8),%edx
	movq	0(%rsp),%r15
	movq	%r8,24(%rsp)
	movq	%r9,32(%rsp)
	movq	%r10,40(%rsp)

	call	_x86_64_AES_encrypt_compact

	movq	24(%rsp),%r8
	movq	32(%rsp),%r9
	movq	40(%rsp),%r10
	movl	%eax,0(%r9)
	movl	%ebx,4(%r9)
	movl	%ecx,8(%r9)
	movl	%edx,12(%r9)

	leaq	16(%r8),%r8
	leaq	16(%r9),%r9
	subq	$16,%r10
	testq	$-16,%r10
	jnz	.Lcbc_slow_enc_loop
	testq	$15,%r10
	jnz	.Lcbc_slow_enc_tail
	movq	56(%rsp),%rbp
	movl	%eax,0(%rbp)
	movl	%ebx,4(%rbp)
	movl	%ecx,8(%rbp)
	movl	%edx,12(%rbp)

	jmp	.Lcbc_exit

.align	4
.Lcbc_slow_enc_tail:
	movq	%rax,%r11
	movq	%rcx,%r12
	movq	%r10,%rcx
	movq	%r8,%rsi
	movq	%r9,%rdi
.long	0xFF
	movq	$16,%rcx
	subq	%r10,%rcx
	xorq	%rax,%rax
.long	0xFF
	movq	%r9,%r8
	movq	$16,%r10
	movq	%r11,%rax
	movq	%r12,%rcx
	jmp	.Lcbc_slow_enc_loop

.align	16
.LSLOW_DECRYPT:
	shrq	$3,%rax
	addq	%rax,%r14

	movq	0(%rbp),%r11
	movq	8(%rbp),%r12
	movq	%r11,0+64(%rsp)
	movq	%r12,8+64(%rsp)

.align	4
.Lcbc_slow_dec_loop:
	movl	0(%r8),%eax
	movl	4(%r8),%ebx
	movl	8(%r8),%ecx
	movl	12(%r8),%edx
	movq	0(%rsp),%r15
	movq	%r8,24(%rsp)
	movq	%r9,32(%rsp)
	movq	%r10,40(%rsp)

	call	_x86_64_AES_decrypt_compact

	movq	24(%rsp),%r8
	movq	32(%rsp),%r9
	movq	40(%rsp),%r10
	xorl	0+64(%rsp),%eax
	xorl	4+64(%rsp),%ebx
	xorl	8+64(%rsp),%ecx
	xorl	12+64(%rsp),%edx

	movq	0(%r8),%r11
	movq	8(%r8),%r12
	subq	$16,%r10
	jc	.Lcbc_slow_dec_partial
	jz	.Lcbc_slow_dec_done

	movq	%r11,0+64(%rsp)
	movq	%r12,8+64(%rsp)

	movl	%eax,0(%r9)
	movl	%ebx,4(%r9)
	movl	%ecx,8(%r9)
	movl	%edx,12(%r9)

	leaq	16(%r8),%r8
	leaq	16(%r9),%r9
	jmp	.Lcbc_slow_dec_loop
.Lcbc_slow_dec_done:
	movq	56(%rsp),%rdi
	movq	%r11,0(%rdi)
	movq	%r12,8(%rdi)

	movl	%eax,0(%r9)
	movl	%ebx,4(%r9)
	movl	%ecx,8(%r9)
	movl	%edx,12(%r9)

	jmp	.Lcbc_exit

.align	4
.Lcbc_slow_dec_partial:
	movq	56(%rsp),%rdi
	movq	%r11,0(%rdi)
	movq	%r12,8(%rdi)

	movl	%eax,0+64(%rsp)
	movl	%ebx,4+64(%rsp)
	movl	%ecx,8+64(%rsp)
	movl	%edx,12+64(%rsp)

	movq	%r9,%rdi
	leaq	64(%rsp),%rsi
	leaq	16(%r10),%rcx
.long	0xFF
	jmp	.Lcbc_exit

.align	16
.Lcbc_exit:
	movq	16(%rsp),%rsi
.cfi_def_cfa	%rsi,64
	movq	(%rsi),%r15
.cfi_restore	%r15
	movq	8(%rsi),%r14
.cfi_restore	%r14
	movq	16(%rsi),%r13
.cfi_restore	%r13
	movq	24(%rsi),%r12
.cfi_restore	%r12
	movq	32(%rsi),%rbp
.cfi_restore	%rbp
	movq	40(%rsi),%rbx
.cfi_restore	%rbx
	leaq	48(%rsi),%rsp
.cfi_def_cfa	%rsp,16
.Lcbc_popfq:
	popfq


.cfi_adjust_cfa_offset	-8
.Lcbc_epilogue:
	.byte	0xFF,0xFF
.cfi_endproc	
.size	AES_cbc_encrypt,.-AES_cbc_encrypt
.align	64
.LAES_Te:
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.long	0xFF, 0xFF, 0xFF, 0xFF
.long	0xFF, 0xFF, 0xFF, 0xFF
.long	0xFF, 0xFF, 0xFF, 0xFF
.long	0xFF, 0xFF, 0xFF, 0xFF
.align	64
.LAES_Td:
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.long	0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.long	0xFF, 0xFF, 0xFF, 0xFF
.long	0xFF, 0xFF, 0, 0
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.long	0xFF, 0xFF, 0xFF, 0xFF
.long	0xFF, 0xFF, 0, 0
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.long	0xFF, 0xFF, 0xFF, 0xFF
.long	0xFF, 0xFF, 0, 0
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.byte	0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF
.long	0xFF, 0xFF, 0xFF, 0xFF
.long	0xFF, 0xFF, 0, 0
.byte	65,69,83,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
.align	64
