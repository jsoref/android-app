.text
.globl	fcrypt_body
.type	fcrypt_body,@function
.align	16
fcrypt_body:
.L_fcrypt_body_begin:
	pushl	%ebp
	pushl	%ebx
	pushl	%esi
	pushl	%edi


	xorl	%edi,%edi
	xorl	%esi,%esi
	call	.L000PIC_me_up
.L000PIC_me_up:
	popl	%edx
	leal	_GLOBAL_OFFSET_TABLE_+[.-.L000PIC_me_up](%edx),%edx
	movl	DES_SPtrans@GOT(%edx),%edx
	pushl	%edx
	movl	28(%esp),%ebp
	pushl	$25
.L001start:


	movl	36(%esp),%eax
	movl	%esi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%esi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	(%ebp),%ebx
	xorl	%ebx,%eax
	movl	4(%ebp),%ecx
	xorl	%esi,%eax
	xorl	%esi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%edi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%edi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%edi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%edi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%edi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	8(%ebp),%ebx
	xorl	%ebx,%eax
	movl	12(%ebp),%ecx
	xorl	%edi,%eax
	xorl	%edi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%esi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%esi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%esi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%esi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%esi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	16(%ebp),%ebx
	xorl	%ebx,%eax
	movl	20(%ebp),%ecx
	xorl	%esi,%eax
	xorl	%esi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%edi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%edi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%edi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%edi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%edi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	24(%ebp),%ebx
	xorl	%ebx,%eax
	movl	28(%ebp),%ecx
	xorl	%edi,%eax
	xorl	%edi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%esi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%esi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%esi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%esi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%esi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	32(%ebp),%ebx
	xorl	%ebx,%eax
	movl	36(%ebp),%ecx
	xorl	%esi,%eax
	xorl	%esi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%edi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%edi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%edi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%edi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%edi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	40(%ebp),%ebx
	xorl	%ebx,%eax
	movl	44(%ebp),%ecx
	xorl	%edi,%eax
	xorl	%edi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%esi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%esi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%esi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%esi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%esi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	48(%ebp),%ebx
	xorl	%ebx,%eax
	movl	52(%ebp),%ecx
	xorl	%esi,%eax
	xorl	%esi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%edi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%edi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%edi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%edi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%edi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	56(%ebp),%ebx
	xorl	%ebx,%eax
	movl	60(%ebp),%ecx
	xorl	%edi,%eax
	xorl	%edi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%esi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%esi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%esi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%esi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%esi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	64(%ebp),%ebx
	xorl	%ebx,%eax
	movl	68(%ebp),%ecx
	xorl	%esi,%eax
	xorl	%esi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%edi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%edi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%edi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%edi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%edi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	72(%ebp),%ebx
	xorl	%ebx,%eax
	movl	76(%ebp),%ecx
	xorl	%edi,%eax
	xorl	%edi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%esi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%esi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%esi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%esi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%esi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	80(%ebp),%ebx
	xorl	%ebx,%eax
	movl	84(%ebp),%ecx
	xorl	%esi,%eax
	xorl	%esi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%edi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%edi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%edi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%edi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%edi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	88(%ebp),%ebx
	xorl	%ebx,%eax
	movl	92(%ebp),%ecx
	xorl	%edi,%eax
	xorl	%edi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%esi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%esi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%esi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%esi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%esi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	96(%ebp),%ebx
	xorl	%ebx,%eax
	movl	100(%ebp),%ecx
	xorl	%esi,%eax
	xorl	%esi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%edi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%edi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%edi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%edi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%edi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	104(%ebp),%ebx
	xorl	%ebx,%eax
	movl	108(%ebp),%ecx
	xorl	%edi,%eax
	xorl	%edi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%esi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%esi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%esi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%esi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%esi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	112(%ebp),%ebx
	xorl	%ebx,%eax
	movl	116(%ebp),%ecx
	xorl	%esi,%eax
	xorl	%esi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%edi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%edi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%edi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%edi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%edi
	movl	32(%esp),%ebp


	movl	36(%esp),%eax
	movl	%edi,%edx
	shrl	$16,%edx
	movl	40(%esp),%ecx
	xorl	%edi,%edx
	andl	%edx,%eax
	andl	%ecx,%edx
	movl	%eax,%ebx
	shll	$16,%ebx
	movl	%edx,%ecx
	shll	$16,%ecx
	xorl	%ebx,%eax
	xorl	%ecx,%edx
	movl	120(%ebp),%ebx
	xorl	%ebx,%eax
	movl	124(%ebp),%ecx
	xorl	%edi,%eax
	xorl	%edi,%edx
	xorl	%ecx,%edx
	andl	$0xFF,%eax
	xorl	%ebx,%ebx
	andl	$0xFF,%edx
	xorl	%ecx,%ecx
	movb	%al,%bl
	movb	%ah,%cl
	rorl	$4,%edx
	movl	4(%esp),%ebp
	xorl	(%ebp,%ebx,1),%esi
	movb	%dl,%bl
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	shrl	$16,%eax
	xorl	0xFF(%ebp,%ebx,1),%esi
	movb	%ah,%bl
	shrl	$16,%edx
	xorl	0xFF(%ebp,%ecx,1),%esi
	movb	%dh,%cl
	andl	$0xFF,%eax
	andl	$0xFF,%edx
	movl	0xFF(%ebp,%ebx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%ecx,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%eax,1),%ebx
	xorl	%ebx,%esi
	movl	0xFF(%ebp,%edx,1),%ebx
	xorl	%ebx,%esi
	movl	32(%esp),%ebp
	movl	(%esp),%ebx
	movl	%edi,%eax
	decl	%ebx
	movl	%esi,%edi
	movl	%eax,%esi
	movl	%ebx,(%esp)
	jnz	.L001start


	movl	28(%esp),%edx
	rorl	$1,%edi
	movl	%esi,%eax
	xorl	%edi,%esi
	andl	$0xFF,%esi
	xorl	%esi,%eax
	xorl	%esi,%edi

	roll	$23,%eax
	movl	%eax,%esi
	xorl	%edi,%eax
	andl	$0xFF,%eax
	xorl	%eax,%esi
	xorl	%eax,%edi

	roll	$10,%esi
	movl	%esi,%eax
	xorl	%edi,%esi
	andl	$0xFF,%esi
	xorl	%esi,%eax
	xorl	%esi,%edi

	roll	$18,%edi
	movl	%edi,%esi
	xorl	%eax,%edi
	andl	$0xFF,%edi
	xorl	%edi,%esi
	xorl	%edi,%eax

	roll	$12,%esi
	movl	%esi,%edi
	xorl	%eax,%esi
	andl	$0xFF,%esi
	xorl	%esi,%edi
	xorl	%esi,%eax

	rorl	$4,%eax
	movl	%eax,(%edx)
	movl	%edi,4(%edx)
	addl	$8,%esp
	popl	%edi
	popl	%esi
	popl	%ebx
	popl	%ebp
	ret
.size	fcrypt_body,.-.L_fcrypt_body_begin
